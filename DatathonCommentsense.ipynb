{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glamlytics: L'Oréal MYSG x Monash Datathon CommentSense Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was written by Leticia Lariche for the Glamlytics \n",
    "# Commentsense submission under the L'Oréal MYSG x Monash Datathon\n",
    "\n",
    "# Code Reuse Acknowledgement: Some code from my Monash Unit ECE4179 assignments \n",
    "# were used in this notebook, as the unit touches on neural networks and concepts used here.\n",
    "\n",
    "# AI Use Acknowledgement: ChatGPT was used to brainstorm ideas and debug functions. \n",
    "# Data was never uploaded to the site to prevent confidentiality breaches, \n",
    "# though all the data are publically available metrics.\n",
    "\n",
    "# Outside Source Acknowledgement: DistilBERT was used in training several models: \n",
    "# https://huggingface.co/docs/transformers/en/model_doc/distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to safely install any new packages into the virtual environment\n",
    "\n",
    "import sys; print(sys.version); print(sys.executable)\n",
    "%pip install --upgrade wikipedia\n",
    "import wikipedia, sys; print(wikipedia.__version__); print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv \n",
    "import os \n",
    "import re \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def seed_all(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "# print(transformer_model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing File Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return val.encode(\"latin1\").decode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "files = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\",\n",
    "    \"videos.csv\"\n",
    "]\n",
    "\n",
    "for fname in files:\n",
    "    df = pd.read_csv(fname, encoding=\"latin1\")\n",
    "    df_fixed = df.applymap(fix_encoding)\n",
    "    outname = fname.replace(\".csv\", \"_fixed.csv\")\n",
    "    df_fixed.to_csv(outname, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Fixed and saved: {outname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of Spam Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINK SPAM REMOVAL\n",
    "\n",
    "input_file = \"Fixed Datasets/comments5_fixed.csv\"\n",
    "spam_file = \"link_spam_new.csv\"\n",
    "not_spam_file = \"comments5_not_spam.csv\"\n",
    "\n",
    "spam_exists = os.path.exists(spam_file)\n",
    "\n",
    "with open(input_file, newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(spam_file, \"a\", newline=\"\", encoding=\"utf-8\") as spam_out, \\\n",
    "     open(not_spam_file, \"w\", newline=\"\", encoding=\"utf-8\") as not_spam_out:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + [\"is_link\"] #, \"filtered\"]\n",
    "\n",
    "    spam_writer = csv.DictWriter(spam_out, fieldnames=fieldnames)\n",
    "    not_spam_writer = csv.DictWriter(not_spam_out, fieldnames=fieldnames)\n",
    "\n",
    "    if not spam_exists:\n",
    "        spam_writer.writeheader()\n",
    "    not_spam_writer.writeheader()\n",
    "\n",
    "    spam_count = 0\n",
    "    not_spam_count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        text = row.get(\"textOriginal\", \"\")\n",
    "        if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "            text = \"\"\n",
    "\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        has_link = (\".com\" in text_lower and (\"http\" in text_lower or \"www\" in text_lower))\n",
    "\n",
    "        row[\"is_link\"] = \"yes\" if has_link else \"no\"\n",
    "\n",
    "        if has_link: # or filtered:\n",
    "            spam_writer.writerow(row)\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            not_spam_writer.writerow(row)\n",
    "            not_spam_count += 1\n",
    "\n",
    "print(f\"Finished. Spam: {spam_count} | Not spam: {not_spam_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indonesian/Malay Language Filtering\n",
    "I filtered these using keywords as langdetect doesn't always have the best results, and I am able to read the language to manually catch more occurances through keyword searching. These were later merged with the other non-english comments and marked as id for Indonesian, though some may be Malay, since the languages are mutually intelligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDO FILTERING \n",
    "\n",
    "input_file = \"comments1_filtered_r3.csv\"\n",
    "spam_file = \"indo_comments.csv\"\n",
    "not_spam_file = \"comments1_filtered.csv\"\n",
    "\n",
    "spam_exists = os.path.exists(spam_file)\n",
    "\n",
    "\n",
    "def apply_rules(text: str) > (bool, str):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    indo = [\"gila\", \"baik\", \"cantik\", \"mantap\", \"kakak\", \"banget\", \"sekali\", \n",
    "            \"Masyaallah\", \"Masya Allah\", \"belum\", \"Orang \", \"banyak\", \" itu\", \n",
    "            \"asli\", \"soal\", \"bidadari\", \"²\", \"memang\", \"Wah\", \"cerah\", \n",
    "            \"muka\", \"perempuan\", \"laki\", \"Salut\", \"percaya\", \"lagu\", \n",
    "            \"negeri\", \"negara\", \"ibu \", \"kerana\", \"karena\", \"lucu\", \n",
    "            \"tebal\", \"Alloh\", \"mirip\", \"emang\", \"hadir\"]\n",
    "    if any(indo in text_lower for indo in indo):\n",
    "        return True, \"Indo/Malay\"\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "with open(input_file, newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(spam_file, \"a\", newline=\"\", encoding=\"utf-8\") as spam_out, \\\n",
    "     open(not_spam_file, \"w\", newline=\"\", encoding=\"utf-8\") as not_spam_out:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + [\"spam_reason\"]\n",
    "\n",
    "    spam_writer = csv.DictWriter(spam_out, fieldnames=fieldnames)\n",
    "    not_spam_writer = csv.DictWriter(not_spam_out, fieldnames=fieldnames)\n",
    "\n",
    "    if not spam_exists:\n",
    "        spam_writer.writeheader()\n",
    "    not_spam_writer.writeheader()\n",
    "\n",
    "    spam_count = 0\n",
    "    not_spam_count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        text = row.get(\"textOriginal\", \"\")\n",
    "        if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "            text = \"\"\n",
    "\n",
    "        is_spam, reason = apply_rules(text)\n",
    "        row[\"spam_reason\"] = reason if is_spam else \"\"\n",
    "\n",
    "        if is_spam:\n",
    "            spam_writer.writerow(row)\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            not_spam_writer.writerow(row)\n",
    "            not_spam_count += 1\n",
    "\n",
    "print(f\"Finished. Indo: {spam_count} | Not spam: {not_spam_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langdetect non-english comment removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"comments1_filtered.csv\"\n",
    "english_file = \"comments1_filtered_english.csv\"\n",
    "non_english_file = \"comments_non_english.csv\" \n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"                     \n",
    "    \"\\U0001F600-\\U0001F64F\" \n",
    "    \"\\U0001F300-\\U0001F5FF\" \n",
    "    \"\\U0001F680-\\U0001F6FF\" \n",
    "    \"\\U0001F1E0-\\U0001F1FF\" \n",
    "    \"]\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "handle_pattern = re.compile(r\"@\\w+|#\\w+\")\n",
    "\n",
    "def normalize_for_detection(text: str) > str:\n",
    "    text = emoji_pattern.sub(\"\", text)\n",
    "    text = url_pattern.sub(\"\", text)\n",
    "    text = handle_pattern.sub(\"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_language(text: str) > str:\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return \"unknown\"\n",
    "    cleaned = normalize_for_detection(str(text))\n",
    "    if not cleaned:\n",
    "        return \"unknown\"\n",
    "    try:\n",
    "        return detect(cleaned)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "with open(input_file, newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(english_file, \"a\", newline=\"\", encoding=\"utf-8\") as eng_out, \\\n",
    "     open(non_english_file, \"a\", newline=\"\", encoding=\"utf-8\") as non_eng_out:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    base_fields = reader.fieldnames or []\n",
    "    fieldnames = base_fields if \"language\" in base_fields else base_fields + [\"language\"]\n",
    "\n",
    "    eng_writer = csv.DictWriter(eng_out, fieldnames=fieldnames)\n",
    "    non_eng_writer = csv.DictWriter(non_eng_out, fieldnames=fieldnames)\n",
    "\n",
    "    if not non_eng_writer:\n",
    "        non_eng_writer.writeheader()\n",
    "    if not eng_writer:\n",
    "        eng_writer.writeheader()\n",
    "\n",
    "    eng_count = 0\n",
    "    non_eng_count = 0\n",
    "    count = 0\n",
    "\n",
    "    for count, row in enumerate(reader, start=1):\n",
    "        text = row.get(\"textOriginal\", \"\")\n",
    "        lang = detect_language(text)\n",
    "\n",
    "        row[\"language\"] = lang\n",
    "\n",
    "        if lang == \"en\":\n",
    "            eng_writer.writerow(row)\n",
    "            eng_count += 1\n",
    "        else:\n",
    "            non_eng_writer.writerow(row)\n",
    "            non_eng_count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"{count:,} rows completed\", flush=True)\n",
    "\n",
    "print(f\"Done. English: {eng_count} | Non-English: {non_eng_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMOJI SPAM REMOVAL\n",
    "\n",
    "input_file = \"comments5_not_spam.csv\"\n",
    "spam_file = \"emoji_spam_new.csv\"\n",
    "not_spam_file = \"comments5_not_spam_or_emojis.csv\"\n",
    "\n",
    "spam_exists = os.path.exists(spam_file)\n",
    "\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"                     \n",
    "    \"\\U0001F600-\\U0001F64F\" \n",
    "    \"\\U0001F300-\\U0001F5FF\" \n",
    "    \"\\U0001F680-\\U0001F6FF\" \n",
    "    \"\\U0001F1E0-\\U0001F1FF\" \n",
    "    \"]\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def count_emojis(text: str) > int:\n",
    "    return len(emoji_pattern.findall(text))\n",
    "\n",
    "def count_words(text: str) > int:\n",
    "    return len(text.split())\n",
    "\n",
    "def has_emoji_spam(text: str) > bool:\n",
    "    word_count = count_words(text)\n",
    "    emoji_count = count_emojis(text)\n",
    "\n",
    "    return word_count < 3 and emoji_count > 5\n",
    "\n",
    "with open(input_file, newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(spam_file, \"a\", newline=\"\", encoding=\"utf-8\") as spam_out, \\\n",
    "     open(not_spam_file, \"w\", newline=\"\", encoding=\"utf-8\") as not_spam_out:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + [\"is_emoji_spam\"]\n",
    "\n",
    "    spam_writer = csv.DictWriter(spam_out, fieldnames=fieldnames)\n",
    "    not_spam_writer = csv.DictWriter(not_spam_out, fieldnames=fieldnames)\n",
    "\n",
    "    if not spam_exists:\n",
    "        spam_writer.writeheader()\n",
    "    not_spam_writer.writeheader()\n",
    "\n",
    "    spam_count = 0\n",
    "    not_spam_count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        text = row.get(\"textOriginal\", \"\")\n",
    "        if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "            text = \"\"\n",
    "\n",
    "        emoji_spam = has_emoji_spam(text)\n",
    "        row[\"is_emoji_spam\"] = \"yes\" if emoji_spam else \"no\"\n",
    "\n",
    "        if emoji_spam:\n",
    "            spam_writer.writerow(row)\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            not_spam_writer.writerow(row)\n",
    "            not_spam_count += 1\n",
    "\n",
    "print(f\"Finished. Emoji spam: {spam_count} | Not spam: {not_spam_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEYWORDS SPAM REMOVAL\n",
    "\n",
    "input_file = \"comments1_filtered_r2.csv\"\n",
    "spam_file = \"keywords_spam_r2.csv\"\n",
    "not_spam_file = \"comments1_filtered_r3.csv\"\n",
    "\n",
    "spam_exists = os.path.exists(spam_file)\n",
    "\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"                     \n",
    "    \"\\U0001F600-\\U0001F64F\" \n",
    "    \"\\U0001F300-\\U0001F5FF\" \n",
    "    \"\\U0001F680-\\U0001F6FF\" \n",
    "    \"\\U0001F1E0-\\U0001F1FF\" \n",
    "    \"]\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def count_emojis(text: str) > int:\n",
    "    return len(emoji_pattern.findall(text))\n",
    "\n",
    "def count_words(text: str) > int:\n",
    "    return len(text.split())\n",
    "\n",
    "def has_emoji_spam(text: str) > bool:\n",
    "    return count_words(text) < 2 and count_emojis(text) > 3\n",
    "\n",
    "word_pattern = re.compile(r\"[A-Za-z0-9]+\")\n",
    "\n",
    "def count_real_words(text: str) > int:\n",
    "    return len(word_pattern.findall(text))\n",
    "\n",
    "def only_emojis(text: str) > bool:\n",
    "    return count_real_words(text) == 0 and count_emojis(text) > 1\n",
    "\n",
    "def apply_rules(text: str) > (bool, str):\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # 1) Contains \".com\"\n",
    "    if \".com\" in text_lower:\n",
    "        return True, \"contains .com\"\n",
    "\n",
    "    # 2) Contains \"our website\"\n",
    "    if \" our website\" in text_lower:\n",
    "        return True, \"contains 'our website'\"\n",
    "\n",
    "    # 3) Countries with no context \n",
    "    countries = [\"india\", \"Lndia\", \"china\",\"indonesia\",\"indo\",\"pakistan\",\n",
    "                 \"nigeria\",\"brazil\",\"bangladesh\",\"russie\",\"ethiopia\",\n",
    "                 \"mexico\",\"japan\",\"china\",\"egypt\",\"philippines\",\"vietnam\",\n",
    "                 \"iran\",\"turkey\",\"germany\",\"thailand\",\"the UK\",\"france\",\n",
    "                 \"south africa\",\"italy\",\"kenya\",\"myanmar\",\"spain\", \"america\", \"korea\"]\n",
    "\n",
    "    if any(country in text_lower for country in countries):\n",
    "        if count_words(text) <= 3 and \"love\" not in text_lower:\n",
    "            return True, \"Country pride\"\n",
    "\n",
    "    # 4) more emoji spam\n",
    "    if has_emoji_spam(text) or only_emojis(text):\n",
    "        return True, \"emoji spam\"\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "with open(input_file, newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(spam_file, \"a\", newline=\"\", encoding=\"utf-8\") as spam_out, \\\n",
    "     open(not_spam_file, \"w\", newline=\"\", encoding=\"utf-8\") as not_spam_out:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + [\"spam_reason\"]\n",
    "\n",
    "    spam_writer = csv.DictWriter(spam_out, fieldnames=fieldnames)\n",
    "    not_spam_writer = csv.DictWriter(not_spam_out, fieldnames=fieldnames)\n",
    "\n",
    "    if not spam_exists:\n",
    "        spam_writer.writeheader()\n",
    "    not_spam_writer.writeheader()\n",
    "\n",
    "    spam_count = 0\n",
    "    not_spam_count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        text = row.get(\"textOriginal\", \"\")\n",
    "        if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "            text = \"\"\n",
    "\n",
    "        is_spam, reason = apply_rules(text)\n",
    "        row[\"spam_reason\"] = reason if is_spam else \"\"\n",
    "\n",
    "        if is_spam:\n",
    "            spam_writer.writerow(row)\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            not_spam_writer.writerow(row)\n",
    "            not_spam_count += 1\n",
    "\n",
    "print(f\"Finished. Spam: {spam_count} | Not spam: {not_spam_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will categorise them based on keyword hits \n",
    "# for top terms in each category\n",
    "# From here, we will train the CNN to recognise \n",
    "# titles in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VIDEOS = \"videos_fixed.csv\"\n",
    "OUTPUT_HITS = \"videos_with_tags.csv\"\n",
    "OUTPUT_NONHITS = \"videos_no_tags.csv\"\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"makeup\": [\n",
    "        r\"\\bmake[-\\s]?up\\b\", r\"\\bgrwm\\b\", r\"\\bglam\\b\", r\"\\bprimer\\b\", r\"\\bfoundation\\b\",\n",
    "        r\"\\bconcealer\\b\", r\"\\bblush\\b\", r\"\\bbronzer\\b\", r\"\\bcontour\\b\", r\"\\bhighlighter\\b\",\n",
    "        r\"\\bsetting (?:spray|powder)\\b\", r\"\\bpowder\\b\", r\"\\bpalette\\b\", r\"\\beyeshadow\\b\",\n",
    "        r\"\\bmascara\\b\", r\"\\beyeliner\\b\", r\"\\bbrow(?:s| gel| pencil| soap)?\\b\",\n",
    "        r\"\\blip(?:stick| gloss| oil| liner)?\\b\",\n",
    "        r\"\\bmakeup\\s+(?:tutorial|routine|haul|dupe|review)\\b\",\n",
    "    ],\n",
    "    \"skincare\": [\n",
    "        r\"\\bskin[-\\s]?care\\b\", r\"\\bskincare\\b\", r\"\\bglowing skin\\b\", r\"\\banti[-\\s]?aging\\b\",\n",
    "        r\"\\bretino(?:l|id)s?\\b\", r\"\\bniacinamide\\b\", r\"\\bvitamin\\s*c\\b\", r\"\\bhyaluronic acid\\b\",\n",
    "        r\"\\bceramide[s]?\\b\", r\"\\bcleanser\\b\", r\"\\bmoisturi[sz]er\\b\", r\"\\bsunscreen\\b\", r\"\\bspf\\b\",\n",
    "        r\"\\btoner\\b\", r\"\\bserum\\b\", r\"\\bessence\\b\", r\"\\bexfoliat(?:e|or|ing)\\b\",\n",
    "        r\"\\b(?:aha|bha|pha)\\b\", r\"\\bsalicylic\\b\", r\"\\bglycolic\\b\", r\"\\bacne\\b|\\bpimple\\b|\\bpores?\\b\",\n",
    "        r\"\\bslugging\\b\", r\"\\bskin(?!ny)\\b\",\n",
    "    ],\n",
    "    \"fragrance\": [\n",
    "        r\"\\bfragrance\\b\", r\"\\bperfume\\b\", r\"\\bparfum\\b\", r\"\\bcologne\\b\",\n",
    "        r\"\\beau de (?:parfum|toilette)\\b\", r\"\\b(?:edp|edt)\\b\", r\"\\bbody mist\\b\",\n",
    "        r\"\\bsillage\\b\", r\"\\bnotes?\\b\", r\"\\bdupe\\b.*\\b(perfume|fragrance)\\b\",\n",
    "    ],\n",
    "    \"hair\": [\n",
    "        r\"\\bhair\\b\", r\"\\bshampoo\\b\", r\"\\bconditioner\\b\", r\"\\bleave[-\\s]?in\\b\",\n",
    "        r\"\\bhair serum\\b\", r\"\\bheat protect(?:ant)?\\b\",\n",
    "        r\"\\b(?:dye|dyeing|color(?:ing)?|bleach|toner)\\b\",\n",
    "        r\"\\bombre\\b|\\bbalayage\\b|\\bhighlights?\\b\",\n",
    "        r\"\\bcurl(?:s|ing)?\\b|\\bcurl(?:er|ing iron)\\b|\\bperm\\b\",\n",
    "        r\"\\bstraighten(?:ing)?\\b|\\bflat iron\\b|\\bblowout\\b|\\bkeratin\\b\",\n",
    "        r\"\\bbraid(?:s|ing)?\\b|\\bwig\\b|\\bweave\\b|\\bextensions?\\b\",\n",
    "        r\"\\bsalon\\b|\\bhaircut\\b|\\bfringe\\b|\\bbangs\\b\",\n",
    "    ],\n",
    "    \"skills\": [\n",
    "        r\"\\bediting\\b\", r\"\\bedit\\b\", r\"\\bcapcut\\b\", r\"\\bpremiere pro\\b\", r\"\\bfinal cut\\b\",\n",
    "        r\"\\bafter effects\\b\", r\"\\bcolor grading\\b\", r\"\\bthumbnail\\b\", r\"\\btransition[s]?\\b\",\n",
    "        r\"\\bworkflow\\b\", r\"\\bfilming\\b\", r\"\\blighting\\b\", r\"\\bb[-\\s]?roll\\b\",\n",
    "    ],\n",
    "    \"nails\": [\n",
    "        r\"\\bnails?\\b\", r\"\\bnail polish\\b\", r\"\\bmani(?:cure)?\\b\", r\"\\bgel[-\\s]?x\\b|\\bgelx\\b\",\n",
    "        r\"\\bgel\\b\", r\"\\bacrylics?\\b\", r\"\\bshellac\\b\", r\"\\bpress[-\\s]?ons?\\b\", r\"\\bnail art\\b\",\n",
    "    ],\n",
    "    \"fashion\": [\n",
    "        r\"\\bfashion\\b\", r\"\\boutfit[s]?\\b|\\bootd\\b|\\bfit check\\b\", r\"\\bclothes\\b|\\bwardrobe\\b\",\n",
    "        r\"\\bpetite\\b\", r\"\\bhaul\\b|\\btry[-\\s]?on\\b|\\blookbook\\b|\\bcapsule\\b\",\n",
    "        r\"\\bworkout set\\b|\\bsports bra\\b|\\bleggings\\b\",\n",
    "        r\"\\bjeans\\b|\\bdress\\b|\\bheels\\b|\\bsneakers\\b\",\n",
    "        r\"\\bstyling\\b|\\btrend[s]?\\b|\\baesthetic\\b|\\bstreetwear\\b\",\n",
    "        r\"\\bdupe\\b.*\\b(outfit|clothes|fashion)\\b\",\n",
    "    ],\n",
    "    \"general lifestyle\": [\n",
    "        r\"\\bvlog\\b\", r\"\\bday in (?:my|the) life\\b\", r\"\\bweek in (?:my|the) life\\b\",\n",
    "        r\"\\bmorning routine\\b|\\bnight routine\\b|\\broutine\\b|\\breset\\b\",\n",
    "        r\"\\bself[-\\s]?care\\b\", r\"\\bproductivity\\b\", r\"\\bclean with me\\b\",\n",
    "        r\"\\bget ready with me\\b|\\bgrwm\\b\", r\"\\bdeclutter\\b\", r\"\\borganize\\b\",\n",
    "        r\"\\bhome\\b|\\bapartment\\b|\\bdecor(?:ating|)\\b\", r\"\\bgrocery\\b|\\bmeal prep\\b\",\n",
    "    ],\n",
    "    # \"misc\": [\n",
    "    #     r\"\\bq\\s*&\\s*a\\b|\\bq and a\\b|\\bqa\\b\",\n",
    "    #     r\"\\basmr\\b\", r\"\\bgiveaway\\b\", r\"\\bannouncement\\b\", r\"\\bstorytime\\b\",\n",
    "    # ],\n",
    "}\n",
    "\n",
    "def compile_category_patterns(categories: dict):\n",
    "    return {cat: [re.compile(p, re.IGNORECASE) for p in pats] for cat, pats in categories.items()}\n",
    "\n",
    "def count_hits(title: str, rx_list):\n",
    "    if not isinstance(title, str) or not title:\n",
    "        return 0\n",
    "    return sum(len(rx.findall(title)) for rx in rx_list)\n",
    "\n",
    "def tag_title(title: str, compiled_pats: dict, category_order=None):\n",
    "    hits = {cat: count_hits(title, rxs) for cat, rxs in compiled_pats.items()}\n",
    "    order = category_order or list(compiled_pats.keys())\n",
    "    tags = [cat for cat in order if hits.get(cat, 0) > 0]\n",
    "    return hits, \"; \".join(tags) if tags else \"\"\n",
    "\n",
    "df = pd.read_csv(INPUT_VIDEOS, engine=\"python\", on_bad_lines=\"skip\", dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "compiled = compile_category_patterns(CATEGORIES)\n",
    "\n",
    "hit_cols = [f\"hits_{cat.replace(' ', '_')}\" for cat in CATEGORIES]\n",
    "for col in hit_cols:\n",
    "    df[col] = 0\n",
    "\n",
    "tags_out = []\n",
    "for i, title in enumerate(df[\"title\"].fillna(\"\").astype(str)):\n",
    "    hits, tags = tag_title(title, compiled, category_order=list(CATEGORIES.keys()))\n",
    "    for cat, h in hits.items():\n",
    "        df.at[i, f\"hits_{cat.replace(' ', '_')}\"] = h\n",
    "    tags_out.append(tags)\n",
    "df[\"tags\"] = tags_out  \n",
    "\n",
    "has_hit = df[hit_cols].sum(axis=1) > 0\n",
    "df_hits = df.loc[has_hit].copy()\n",
    "df_non  = df.loc[~has_hit].copy()\n",
    "\n",
    "df_hits.to_csv(OUTPUT_HITS, index=False, encoding=\"utf-8\")\n",
    "df_non.to_csv(OUTPUT_NONHITS, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote {len(df_hits):,} videos with ≥1 category hit to {OUTPUT_HITS}\")\n",
    "print(f\"Wrote {len(df_non):,} videos with no hits to {OUTPUT_NONHITS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept everything with hits in only 1 category \n",
    "# as the golden standard\n",
    "\n",
    "INPUT = \"videos_with_tags_defining.csv\"\n",
    "NO_TAGS_OUTPUT = \"videos_no_tags.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT, engine=\"python\", on_bad_lines=\"skip\", dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "def_col = next(\n",
    "    (c for c in df.columns if re.sub(r\"\\s+\", \"\", c.strip().lower()) == \"definingcategory\"),\n",
    "    None\n",
    ")\n",
    "\n",
    "mask_no = df[def_col].isna() | df[def_col].astype(str).str.strip().eq(\"\")\n",
    "df_no   = df.loc[mask_no].copy()\n",
    "df_keep = df.loc[~mask_no].copy()\n",
    "need_header = not Path(NO_TAGS_OUTPUT).exists() or Path(NO_TAGS_OUTPUT).stat().st_size == 0\n",
    "if not df_no.empty:\n",
    "    df_no.to_csv(NO_TAGS_OUTPUT, mode=\"a\", index=False, encoding=\"utf-8\", header=need_header)\n",
    "\n",
    "df_keep.to_csv(INPUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"Total rows in input: {len(df):,}\")\n",
    "print(f\"Appended to '{NO_TAGS_OUTPUT}': {len(df_no):,} rows without a defining category.\")\n",
    "print(f\"Remaining in '{INPUT}': {len(df_keep):,} rows with a defining category.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we match the videos to the comments so we \n",
    "# can have the video and comment category together\n",
    "\n",
    "\n",
    "COMMENT_FILES = [\n",
    "    \"comments1_filtered_english.csv\",\n",
    "    \"comments2_filtered_english.csv\",\n",
    "    \"comments3_filtered_english.csv\",\n",
    "    \"comments4_filtered_english.csv\",\n",
    "    \"comments5_filtered_english.csv\",\n",
    "]\n",
    "VIDEOS_CSV = \"videos_fixed.csv\"\n",
    "OUTPUT_SUFFIX = \"_with_video_details.csv\"\n",
    "\n",
    "READ_KW = dict(engine=\"python\", on_bad_lines=\"skip\", dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "COMMENT_ORDER = [\"kind\",\"commentId\",\"channelId\",\"videoId\",\"authorId\",\n",
    "                 \"textOriginal\",\"parentCommentId\",\"likeCount\",\"publishedAt\",\"updatedAt\"]\n",
    "VIDEO_ORDER   = [\"kind\",\"videoId\",\"publishedAt\",\"channelId\",\"title\",\"description\",\"tags\",\n",
    "                 \"defaultLanguage\",\"defaultAudioLanguage\",\"contentDuration\",\"viewCount\",\n",
    "                 \"likeCount\",\"favouriteCount\",\"commentCount\",\"topicCategories\"]\n",
    "\n",
    "videos = pd.read_csv(VIDEOS_CSV, **READ_KW)\n",
    "videos = videos.drop_duplicates(subset=[\"videoId\"], keep=\"first\")\n",
    "ordered_video_cols = [c for c in VIDEO_ORDER if c in videos.columns] + \\\n",
    "                     [c for c in videos.columns if c not in VIDEO_ORDER]\n",
    "video_rename = {c: f\"video_{c}\" for c in ordered_video_cols if c != \"videoId\"}\n",
    "videos_pref = videos.rename(columns=video_rename)\n",
    "\n",
    "\n",
    "for cpath in COMMENT_FILES:\n",
    "    if not os.path.exists(cpath):\n",
    "        print(f\"Warning: {cpath} not found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    comments = pd.read_csv(cpath, **READ_KW)\n",
    "    if \"videoId\" not in comments.columns:\n",
    "        print(f\"Warning: {cpath} missing 'videoId'; writing unchanged.\")\n",
    "        out_df = comments\n",
    "    else:\n",
    "        merged = comments.merge(videos_pref, on=\"videoId\", how=\"left\", copy=False)\n",
    "        comment_cols_in_file_order = list(comments.columns)\n",
    "        video_cols_pref = [video_rename[c] for c in ordered_video_cols if c != \"videoId\" and video_rename[c] in merged.columns]\n",
    "        final_cols = comment_cols_in_file_order + [c for c in video_cols_pref if c not in comment_cols_in_file_order]\n",
    "\n",
    "        out_df = merged.loc[:, final_cols]\n",
    "\n",
    "    out_path = Path(cpath).with_name(Path(cpath).stem + OUTPUT_SUFFIX)\n",
    "    out_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    probe_col = \"video_title\"\n",
    "    matched = int(out_df[probe_col].notna().sum()) if probe_col in out_df.columns else 0\n",
    "    print(f\"Wrote {len(out_df):,} rows to {out_path.name} | matched videos: {matched:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(s: str) > str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower().replace(\"_\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "class VideoCategoryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        tokenizer,\n",
    "        max_length=128,\n",
    "        title_col=\"title\",\n",
    "        label_col=\"defining category\",    \n",
    "        categories=None,  \n",
    "        **kwargs,\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "        if label_col not in df.columns:\n",
    "            alt = label_col.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "            if alt in df.columns:\n",
    "                label_col = alt\n",
    "            else:\n",
    "                raise ValueError(f\"Expected label column '{label_col}' (or '{alt}') in {csv_file}. Found: {list(df.columns)}\")\n",
    "\n",
    "        if title_col not in df.columns:\n",
    "            raise ValueError(f\"Expected title column '{title_col}' in {csv_file}. Found: {list(df.columns)}\")\n",
    "\n",
    "        if categories is None:\n",
    "            categories = [\"makeup\",\"skincare\",\"fragrance\",\"hair\",\"skills\",\"nails\",\"fashion\",\"general lifestyle\"]\n",
    "\n",
    "        self.class_names = categories[:]\n",
    "        canon_classes = [_norm(c) for c in categories]\n",
    "        self.label2id = {c: i for i, c in enumerate(canon_classes)}\n",
    "        self.id2label = {i: self.class_names[i] for i in range(len(categories))}\n",
    "        self.num_labels = len(categories)\n",
    "\n",
    "        df[title_col] = df[title_col].fillna(\"\").astype(str)\n",
    "        df[\"_label_norm\"] = df[label_col].apply(_norm)\n",
    "\n",
    "        mask_valid = (df[title_col].str.strip() != \"\") & (df[\"_label_norm\"].isin(canon_classes))\n",
    "        df = df[mask_valid]\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"No usable rows after filtering, check columns and categories\")\n",
    "\n",
    "        df[\"_label_id\"] = df[\"_label_norm\"].map(self.label2id).astype(int)\n",
    "\n",
    "        self.titles = df[title_col].tolist()\n",
    "        self.labels = df[\"_label_id\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        counts = df[\"_label_id\"].value_counts().reindex(range(self.num_labels), fill_value=0)\n",
    "        freq = counts.to_numpy()\n",
    "        with torch.no_grad():\n",
    "            inv = torch.tensor([0.0 if f == 0 else 1.0 / f for f in freq], dtype=torch.float)\n",
    "            self.class_weights = (inv * (self.num_labels / inv.sum())) if inv.sum() > 0 else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        label = self.labels[idx]  \n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            title,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return input_ids, attention_mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 48\n",
    "batch_size = 32\n",
    "\n",
    "SOURCE     = \"videos_with_tags_defining.csv\"\n",
    "TRAIN_OUT  = \"tags_train.csv\"\n",
    "TEST_OUT   = \"tags_trainer_test.csv\"\n",
    "\n",
    "LABEL_COL  = \"defining_category\"\n",
    "TITLE_COL  = \"title\"\n",
    "\n",
    "df = pd.read_csv(SOURCE, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "\n",
    "label_norm = (\n",
    "    df[LABEL_COL].astype(str)\n",
    "      .str.strip().str.lower()\n",
    "      .str.replace(\"_\", \" \")\n",
    "      .str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "has_title = df[TITLE_COL].astype(str).str.strip() != \"\"\n",
    "mask = label_norm.ne(\"\") & label_norm.notna() & has_title\n",
    "\n",
    "df_clean = df.loc[mask].copy()\n",
    "y = label_norm.loc[mask]\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df_clean, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_OUT, index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(TEST_OUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(train_df):,} rows to {TRAIN_OUT} and {len(test_df):,} rows to {TEST_OUT}\")\n",
    "\n",
    "CATS = [\"makeup\",\"skincare\",\"fragrance\",\"hair\",\"skills\",\"nails\",\"fashion\",\"general lifestyle\"]\n",
    "\n",
    "dataset_train = VideoCategoryDataset(\n",
    "    TRAIN_OUT, tokenizer=tokenizer, max_length=max_length,\n",
    "    title_col=TITLE_COL, label_col=LABEL_COL, categories=CATS\n",
    ")\n",
    "dataset_test  = VideoCategoryDataset(\n",
    "    TEST_OUT, tokenizer, max_length=max_length,\n",
    "    title_col=TITLE_COL, label_col=LABEL_COL, categories=CATS\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "loader_test  = DataLoader(dataset_test,  batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCategoriser(nn.Module):\n",
    "\n",
    "    def __init__(self, transformer, num_class, device='cpu',\n",
    "                 freeze_transformer=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformer = transformer\n",
    "        self.hidden = transformer.config.hidden_size\n",
    "\n",
    "        if freeze_transformer:\n",
    "            for p in self.transformer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden, num_class)\n",
    "\n",
    "        self.to(device=self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        kwargs = dict(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if token_type_ids is not None:\n",
    "            kwargs[\"token_type_ids\"] = token_type_ids\n",
    "        out = self.transformer(**kwargs)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(self.dropout(cls))\n",
    "        return logits\n",
    "\n",
    "    def _unpack_batch(self, batch):\n",
    "        if len(batch) == 3:\n",
    "            input_ids, attention_mask, y = batch\n",
    "            token_type_ids = None\n",
    "        elif len(batch) == 4:\n",
    "            input_ids, attention_mask, token_type_ids, y = batch\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected batch format with {len(batch)} items.\")\n",
    "        return input_ids, attention_mask, token_type_ids, y\n",
    "\n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True, class_weights=None):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log  = []\n",
    "        self.best_loss = float('inf')\n",
    "        best_epoch = -1\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=(class_weights.to(self.device) if class_weights is not None else None)\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            # (1) loop over loader_train\n",
    "            for batch in loader_train:\n",
    "                input_ids, attention_mask, token_type_ids, y = self._unpack_batch(batch)\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                y = y.long().to(self.device)\n",
    "                if token_type_ids is not None:\n",
    "                    token_type_ids = token_type_ids.long().to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                yhat = self.forward(input_ids, attention_mask, token_type_ids)\n",
    "                loss = loss_fn(yhat, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # (2) training loss\n",
    "            loss_train = self.evaluate(loader_train, class_weights=class_weights)\n",
    "            self.loss_train_log.append(loss_train)\n",
    "\n",
    "            # (3) validation loss\n",
    "            loss_test = self.evaluate(loader_test, class_weights=class_weights)\n",
    "            self.loss_test_log.append(loss_test)\n",
    "\n",
    "            # (4) print progress\n",
    "            if verbose:\n",
    "                print('Epochs %d/%d' % (epoch+1, epochs))\n",
    "                print('Train Loss = %.4f' % loss_train, end=', ')\n",
    "                print('Val Loss = %.4f' % loss_test)\n",
    "\n",
    "            # (5) save best\n",
    "            if loss_test < self.best_loss:\n",
    "                self.best_loss = loss_test\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.state_dict(), 'video_categoriser_best_params.pt')\n",
    "\n",
    "        print(f'Best model saved at epoch {best_epoch} with loss {self.best_loss:.4f}.')\n",
    "\n",
    "    def evaluate(self, loader, class_weights=None):\n",
    "        self.eval()\n",
    "        loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=(class_weights.to(self.device) if class_weights is not None else None)\n",
    "        )\n",
    "        total_loss, total_n = 0.0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids, attention_mask, token_type_ids, y = self._unpack_batch(batch)\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                y = y.long().to(self.device)\n",
    "                if token_type_ids is not None:\n",
    "                    token_type_ids = token_type_ids.long().to(self.device)\n",
    "\n",
    "                y_pred = self.forward(input_ids, attention_mask, token_type_ids)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                bs = y.size(0)\n",
    "                total_loss += loss.item() * bs\n",
    "                total_n += bs\n",
    "\n",
    "        return total_loss / max(total_n, 1)\n",
    "\n",
    "    def predict(self, loader):\n",
    "        self.eval()\n",
    "        x_all, y_all, logit = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids, attention_mask, token_type_ids, y = self._unpack_batch(batch)\n",
    "                x_all.append(input_ids) \n",
    "                y_all.append(y)\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                if token_type_ids is not None:\n",
    "                    token_type_ids = token_type_ids.long().to(self.device)\n",
    "\n",
    "                yhat = self.forward(input_ids, attention_mask, token_type_ids)\n",
    "                logit.append(yhat.cpu())\n",
    "\n",
    "        x_all  = torch.cat(x_all).cpu()\n",
    "        y_all  = torch.cat(y_all).cpu()\n",
    "        logit  = torch.cat(logit).cpu()\n",
    "        return x_all, y_all, logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "epochs = 5\n",
    "model = VideoCategoriser(transformer=transformer_model, num_class=8, device = 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.Train(epochs, optimizer, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous output: \n",
    "# Epochs 1/5\n",
    "# Train Loss = 0.3614, Val Loss = 0.3713\n",
    "# Epochs 2/5\n",
    "# Train Loss = 0.3217, Val Loss = 0.3334\n",
    "# Epochs 3/5\n",
    "# Train Loss = 0.3221, Val Loss = 0.3417\n",
    "# Epochs 4/5\n",
    "# Train Loss = 0.3361, Val Loss = 0.3500\n",
    "# Epochs 5/5\n",
    "# Train Loss = 0.2884, Val Loss = 0.3070\n",
    "# Best model saved at epoch 5 with loss 0.3070."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = torch.load('video_categoriser_best_params.pt')\n",
    "model_best = VideoCategoriser(device=device, transformer=transformer_model, num_class = 8)\n",
    "model_best.load_state_dict(best_params)\n",
    "x_all, y_all, logit = model_best.predict(loader_test)\n",
    "\n",
    "probs = F.softmax(logit, dim=1)\n",
    "y_pred = torch.argmax(probs, dim=1)\n",
    "accuracy = (y_pred == y_all).float().mean().item()\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV   = \"leftover_videos.csv\"\n",
    "OUTPUT_CSV  = \"leftover_videos_tagged.csv\"\n",
    "PRETRAINED  = \"distilbert-base-uncased\"\n",
    "BEST_WEIGHTS = \"video_categoriser_best_params.pt\" \n",
    "MAX_LENGTH  = 128\n",
    "BATCH_SIZE  = 64\n",
    "PRINT_EVERY = 1000\n",
    "\n",
    "CATS        = [\"makeup\",\"skincare\",\"fragrance\",\"hair\",\"skills\",\"nails\",\"fashion\",\"general lifestyle\"]\n",
    "PRETRAINED  = \"distilbert-base-uncased\"\n",
    "BEST_WEIGHTS = \"video_categoriser_best_params.pt\"\n",
    "MAX_LENGTH  = 128\n",
    "BATCH_SIZE  = 64\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# If model/tokenizer aren't already defined in the notebook, quickly load them:\n",
    "if \"tokenizer\" not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "if \"model\" not in globals():\n",
    "    backbone = AutoModel.from_pretrained(PRETRAINED)\n",
    "    model = VideoCategoriser(backbone, num_class=len(CATS), device=device,\n",
    "                             freeze_transformer=True) #, id2label=CATS)\n",
    "    state = torch.load(BEST_WEIGHTS, map_location=\"cpu\")\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "def _slug(c: str) > str:\n",
    "    return c.lower().replace(\" \", \"_\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_probs(titles, descs, batch_size=BATCH_SIZE):\n",
    "    probs_all = []\n",
    "    n = len(titles)\n",
    "    for i in range(0, n, batch_size):\n",
    "        enc = tokenizer(\n",
    "            titles[i:i+batch_size],\n",
    "            text_pair=descs[i:i+batch_size],\n",
    "            truncation=\"only_second\",\n",
    "            padding=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attn      = enc[\"attention_mask\"].to(device)\n",
    "        tti       = enc.get(\"token_type_ids\")\n",
    "        if tti is not None: tti = tti.to(device)\n",
    "\n",
    "        logits = model(input_ids, attn, tti)     # (B, C)\n",
    "        probs  = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "    return np.vstack(probs_all) if probs_all else np.zeros((0, len(CATS)))\n",
    "\n",
    "def tag_no_tags(input_csv=INPUT_CSV, output_csv=OUTPUT_CSV, k_ambiguous=100):\n",
    "    df = pd.read_csv(input_csv, engine=\"python\", on_bad_lines=\"skip\", dtype=str, encoding=\"utf-8\")\n",
    "    \n",
    "    titles = df[\"title\"].fillna(\"\").astype(str).tolist()\n",
    "    descs  = df[\"description\"].fillna(\"\").astype(str).tolist()\n",
    "    probs  = _predict_probs(titles, descs)  # (N, C)\n",
    "\n",
    "    for j, cat in enumerate(CATS):\n",
    "        df[f\"prob_{_slug(cat)}\"] = probs[:, j].round(6)\n",
    "\n",
    "    top3_idx = np.argsort(-probs, axis=1)[:, :3]\n",
    "    n = len(df)\n",
    "    df[\"pred1\"] = [CATS[i] for i in top3_idx[:, 0]]\n",
    "    df[\"pred1_prob\"] = probs[np.arange(n), top3_idx[:, 0]].round(6)\n",
    "    df[\"pred2\"] = [CATS[i] for i in top3_idx[:, 1]]\n",
    "    df[\"pred2_prob\"] = probs[np.arange(n), top3_idx[:, 1]].round(6)\n",
    "    df[\"pred3\"] = [CATS[i] for i in top3_idx[:, 2]]\n",
    "    df[\"pred3_prob\"] = probs[np.arange(n), top3_idx[:, 2]].round(6)\n",
    "\n",
    "    prob_range = probs.max(axis=1) - probs.min(axis=1)\n",
    "    df[\"prob_range_all8\"] = prob_range.round(6)\n",
    "\n",
    "    df[\"defining category\"] = df[\"pred1\"]\n",
    "    k = min(k_ambiguous, n)\n",
    "    ambiguous_idx = np.argsort(prob_range)[:k]\n",
    "    df[\"ambiguous_misc_override\"] = False\n",
    "    df.loc[ambiguous_idx, [\"defining category\", \"ambiguous_misc_override\"]] = [\"misc\", True]\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote {len(df):,} rows with probabilities and tags to '{output_csv}'\")\n",
    "    return df\n",
    "\n",
    "_ = tag_no_tags(INPUT_CSV, OUTPUT_CSV, k_ambiguous=100)\n",
    "\n",
    "#32 mb = 70mins processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Rating\n",
    "Quality: Comments that keep others on the video for longer to like or reply to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [f\"comments{i}_filtered_english.csv\" for i in range(1, 6)]\n",
    "OUTPUT = \"comments_over_1000.csv\"\n",
    "MIN_LIKES = 1000\n",
    "\n",
    "kept = []\n",
    "\n",
    "for path in INPUT_FILES:\n",
    "    try:\n",
    "        df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {path} not found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    if \"likeCount\" not in df.columns:\n",
    "        print(f\"Warning: {path} is missing 'likeCount'; skipping.\")\n",
    "        continue\n",
    "\n",
    "    df[\"likeCount\"] = pd.to_numeric(df[\"likeCount\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    keep = df[df[\"likeCount\"] > MIN_LIKES]\n",
    "    kept.append(keep)\n",
    "    print(f\"{path}: kept {len(keep)} of {len(df)} rows (> {MIN_LIKES})\")\n",
    "\n",
    "if kept:\n",
    "    out = pd.concat(kept, ignore_index=True)\n",
    "\n",
    "    out.to_csv(OUTPUT, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote {len(out)} rows to {OUTPUT}\")\n",
    "else:\n",
    "    print(\"No matching rows found or no valid files read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set top liked comments as high quality, while spam is low quality\n",
    "# and decreases video interest when users read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = [\n",
    "    (\"Quality_scoring/emoji_spam_comments.csv\", \"Poor\"),\n",
    "    (\"Quality_scoring/keywords_spam.csv\", \"Poor\"),\n",
    "    (\"Quality_scoring/link_spam_comments.csv\", \"Poor\"),\n",
    "    (\"Quality_scoring/top_liked_comments.csv\", \"High\"),\n",
    "]\n",
    "\n",
    "OUTPUT = \"quality_trainer.csv\"\n",
    "PREFERRED_COLS = [\"kind\", \"commentId\", \"channelId\", \"videoId\", \"authorId\",\n",
    "                  \"textOriginal\", \"parentCommentId\", \"likeCount\", \"publishedAt\", \"updatedAt\"]\n",
    "\n",
    "POOR_LIMIT = 5500\n",
    "poor_seen = 0\n",
    "dfs = []\n",
    "\n",
    "for path, label in INPUTS:\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\", dtype=str)\n",
    "    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "    if label == \"Poor\":\n",
    "        poor_seen += 1\n",
    "        if poor_seen <= 3 and len(df) > POOR_LIMIT:\n",
    "            df = df.sample(n=POOR_LIMIT, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    df[\"quality\"] = label\n",
    "    dfs.append(df)\n",
    "\n",
    "out = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "cols = [c for c in PREFERRED_COLS if c in out.columns] + [\"quality\"]\n",
    "out = out.loc[:, cols]\n",
    "\n",
    "if \"likeCount\" in out.columns:\n",
    "    lk = pd.to_numeric(out[\"likeCount\"], errors=\"coerce\")\n",
    "    out[\"likeCount\"] = lk.astype(\"Int64\").astype(str)\n",
    "    out.loc[out[\"likeCount\"] == \"<NA>\", \"likeCount\"] = \"\"\n",
    "\n",
    "out.to_csv(OUTPUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(out):,} rows and {len(out.columns)} columns to {OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128,\n",
    "                 text_col=\"textOriginal\", label_col=\"quality\"):\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(csv_file, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "        if text_col not in df.columns or label_col not in df.columns:\n",
    "            raise ValueError(f\"Expected columns '{text_col}' and '{label_col}' in {csv_file}. \"\n",
    "                             f\"Found: {list(df.columns)}\")\n",
    "\n",
    "        # High > 1, Poor > 0\n",
    "        mapping = {\"high\": 1, \"poor\": 0}\n",
    "        df[\"label\"] = df[label_col].str.strip().str.lower().map(mapping)\n",
    "\n",
    "        df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "        df = df[(df[\"label\"].isin([0, 1])) & (df[text_col].str.strip() != \"\")]\n",
    "\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.float).unsqueeze(0)  # shape (1,)\n",
    "\n",
    "        return input_ids, attention_mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "all_df = pd.read_csv(\"quality_trainer.csv\", engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "all_df = all_df[[\"textOriginal\", \"quality\"]].dropna()\n",
    "train_df, test_df = train_test_split(\n",
    "    all_df, test_size=0.2, stratify=all_df[\"quality\"].str.strip().str.lower(), random_state=42\n",
    ")\n",
    "train_df.to_csv(\"quality_trainer_train.csv\", index=False)\n",
    "test_df.to_csv(\"quality_trainer_test.csv\", index=False)\n",
    "\n",
    "dataset_train = QualityDataset(\"quality_trainer_train.csv\", tokenizer, max_length=max_length)\n",
    "dataset_test  = QualityDataset(\"quality_trainer_test.csv\",  tokenizer, max_length=max_length)\n",
    "\n",
    "Q_loader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "Q_loader_test  = DataLoader(dataset_test,  batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScorer(nn.Module):\n",
    "    def __init__(self, transformer, hidden_dim, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.transformer = transformer\n",
    "        transformer_output_dim = transformer.config.hidden_size\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = out.last_hidden_state[:, 0, :]  # CLS token\n",
    "        out = self.classifier(cls_token)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log = []\n",
    "        self.best_loss = np.inf\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train() \n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Step (1)        \n",
    "            for input_ids, attention_mask, labels in loader_train:\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                labels = labels.float().to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels) \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_loss / len(loader_train)\n",
    "            avg_test_loss = self.evaluate(loader_test)\n",
    "\n",
    "            self.loss_train_log.append(avg_train_loss)\n",
    "            self.loss_test_log.append(avg_test_loss)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epochs %d/%d' % (epoch+1, epochs))\n",
    "                print('Train Loss = %.4f' % avg_train_loss, end=', ')\n",
    "                print('Val Loss = %.4f' % avg_test_loss)\n",
    "\n",
    "        # Step (5) save the best model\n",
    "            if avg_test_loss < self.best_loss:\n",
    "                self.best_loss = avg_test_loss\n",
    "                self.best_epoch = epoch + 1\n",
    "                torch.save(self.state_dict(), 'quality_best_params.pt')\n",
    "        \n",
    "        print(f'Best model saved at epoch {self.best_epoch} with loss {self.best_loss:.4f}.')\n",
    "\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.eval() \n",
    "        loss_fn = nn.BCELoss()\n",
    "        loss = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            total_loss = 0\n",
    "\n",
    "            for input_ids, attention_mask, labels in loader:\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                labels = labels.float().to(self.device)\n",
    "\n",
    "                outputs = self.forward(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        loss = total_loss / len(loader)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        self.eval() \n",
    "\n",
    "        x_all, y_all, pred = [], [], [] \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in loader:\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "\n",
    "                outputs = self.forward(input_ids, attention_mask)\n",
    "                preds = (outputs > 0.5).long().cpu().numpy()\n",
    "\n",
    "                x_all.extend(input_ids.cpu().numpy())\n",
    "                y_all.extend(labels.cpu().numpy())\n",
    "                pred.extend(preds)\n",
    "\n",
    "            return x_all, y_all, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_lr = 1e-3\n",
    "Q_epochs = 10\n",
    "Q_model = QualityScorer(transformer = transformer_model, hidden_dim = transformer_model.config.hidden_dim)\n",
    "Q_optimizer = torch.optim.Adam(Q_model.parameters(), lr=lr)\n",
    "Q_model.Train(Q_epochs, Q_optimizer, Q_loader_train, Q_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_all, pred = model.predict(loader_test)\n",
    "y_all = np.array(y_all).flatten()\n",
    "pred = np.array(pred).flatten()\n",
    "accuracy = np.mean(y_all == pred)\n",
    "print(accuracy)\n",
    "\n",
    "# low accuracy of 0.29 but works well on the actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Rating\n",
    "Is the comment positive or negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to limited time, keywords will be used to make the \n",
    "# datasets even though these are not foolproof, \n",
    "# especially with modern slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [\n",
    "    \"comments1_filtered_english.csv\",\n",
    "    \"comments2_filtered_english.csv\",\n",
    "    \"comments3_filtered_english.csv\",\n",
    "    \"comments4_filtered_english.csv\",\n",
    "    \"comments5_filtered_english.csv\",\n",
    "]\n",
    "CHUNK = 10_000\n",
    "PRINT_EVERY = 100000\n",
    "PER_FILE_SUFFIX = \"_positivity_scored.csv\"\n",
    "HITS_OUTPUT = \"positivity_hits_all.csv\"   \n",
    "\n",
    "SCORING_FIELDS = [\"sentiment\", \"pos_hits\", \"neg_hits\", \"source_file\"]\n",
    "\n",
    "def dedup(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "url_re     = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
    "handle_re  = re.compile(r\"[@#]\\w+\", flags=re.IGNORECASE)\n",
    "emoji_re   = re.compile(\"[\"                              \n",
    "    \"\\U0001F600-\\U0001F64F\"  \n",
    "    \"\\U0001F300-\\U0001F5FF\"  \n",
    "    \"\\U0001F680-\\U0001F6FF\"  \n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  \n",
    "    \"]\", flags=re.UNICODE)\n",
    "punct_re   = re.compile(r\"[^\\w\\s]\")\n",
    "space_re   = re.compile(r\"\\s+\")\n",
    "\n",
    "def sanitise(text: str) > str:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    text = url_re.sub(\" \", text)\n",
    "    text = handle_re.sub(\" \", text)\n",
    "    text = emoji_re.sub(\" \", text)\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.lower().replace(\"'\", \"\")  # \"you're\" > \"youre\"\n",
    "    text = punct_re.sub(\" \", text)\n",
    "    text = space_re.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "_positive = [\n",
    "    r\"\\bi love (?:her|him|you|u|ya|this(?: video)?|it)\\b\",\n",
    "    r\"\\blove (?:this|it|you|u|ya|her|him)\\b\",\n",
    "    r\"\\byoure\\s+(?:the\\s+)?best\\b\",\n",
    "    r\"\\bur\\s+(?:the\\s+)?best\\b\",\n",
    "    r\"\\byoure\\s+my\\s+fav(?:ou?rite)\\b\",\n",
    "    r\"\\bur\\s+my\\s+fav(?:ou?rite)\\b\",\n",
    "    r\"\\bmy\\s+fav(?:ou?rite)\\b\",\n",
    "]\n",
    "_negative = [\n",
    "    r\"\\bthis\\s+is\\s+(?:so\\s+)?(?:dumb|stupid|trash|terrible|awful|garbage|cringe|bad)\\b\",\n",
    "    r\"\\b(?:he|she|you|u|they|it)\\s+(?:is|are|so)\\s+(?:dumb|stupid|trash|terrible|awful|garbage|cringe|idiotic)\\b\",\n",
    "    r\"\\b(?:dumb|stupid|idiot|garbage|trash|terrible|awful|cringe)\\b\",\n",
    "    r\"\\bhate\\s+(?:this|it|you|u|her|him)\\b\",\n",
    "    r\"\\bworst\\b\",\n",
    "]\n",
    "POS_RE = [re.compile(p) for p in _positive]\n",
    "NEG_RE = [re.compile(p) for p in _negative]\n",
    "\n",
    "def score_text(text: str):\n",
    "    s = sanitise(text)\n",
    "    pos_hits = sum(1 for rx in POS_RE if rx.search(s))\n",
    "    neg_hits = sum(1 for rx in NEG_RE if rx.search(s))\n",
    "    if pos_hits and not neg_hits:\n",
    "        sentiment = \"positive\"\n",
    "    elif neg_hits and not pos_hits:\n",
    "        sentiment = \"negative\"\n",
    "    elif pos_hits and neg_hits:\n",
    "        sentiment = \"positive\" if pos_hits > neg_hits else \"negative\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    return sentiment, pos_hits, neg_hits\n",
    "\n",
    "\n",
    "union_cols = []\n",
    "seen = set()\n",
    "for path in INPUT_FILES:\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "    try:\n",
    "        cols = list(pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\",\n",
    "                                dtype=str, encoding=\"utf-8\", nrows=0).columns)\n",
    "    except Exception:\n",
    "        with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            cols = next(csv.reader(f), [])\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            union_cols.append(c)\n",
    "\n",
    "union_cols = [c for c in union_cols if c not in SCORING_FIELDS]\n",
    "HITS_FIELDS = dedup(union_cols + SCORING_FIELDS)\n",
    "\n",
    "hits_fh = open(HITS_OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "hits_writer = csv.DictWriter(hits_fh, fieldnames=HITS_FIELDS, extrasaction=\"ignore\")\n",
    "hits_writer.writeheader()\n",
    "\n",
    "\n",
    "for path in INPUT_FILES:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} not found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    in_path = Path(path)\n",
    "    out_path = in_path.with_name(in_path.stem + PER_FILE_SUFFIX)\n",
    "    print(f\"\\nScoring {in_path.name} > {out_path.name}\")\n",
    "\n",
    "    processed = 0\n",
    "    per_file_writer = None\n",
    "    per_file_fh = open(out_path, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "\n",
    "    try:\n",
    "        for chunk in pd.read_csv(\n",
    "            in_path,\n",
    "            engine=\"python\",\n",
    "            on_bad_lines=\"skip\",\n",
    "            dtype=str,\n",
    "            encoding=\"utf-8\",\n",
    "            chunksize=CHUNK,\n",
    "        ):\n",
    "            text_col = \"textOriginal\"\n",
    "\n",
    "            if per_file_writer is None:\n",
    "                file_fields = dedup(list(chunk.columns) + [\"sentiment\", \"pos_hits\", \"neg_hits\"])\n",
    "                per_file_writer = csv.DictWriter(per_file_fh, fieldnames=file_fields, extrasaction=\"ignore\")\n",
    "                per_file_writer.writeheader()\n",
    "\n",
    "            for _, row in chunk.iterrows():\n",
    "                sentiment, pos_hits, neg_hits = score_text(row.get(text_col, \"\"))\n",
    "\n",
    "                out_row = {k: row.get(k, \"\") for k in per_file_writer.fieldnames if k in row.index}\n",
    "                out_row.update({\n",
    "                    \"sentiment\": sentiment,\n",
    "                    \"pos_hits\": pos_hits,\n",
    "                    \"neg_hits\": neg_hits,\n",
    "                })\n",
    "                per_file_writer.writerow(out_row)\n",
    "\n",
    "                if sentiment != \"neutral\":\n",
    "                    hits_row = {k: row.get(k, \"\") for k in union_cols}\n",
    "                    hits_row.update({\n",
    "                        \"sentiment\": sentiment,\n",
    "                        \"pos_hits\": pos_hits,\n",
    "                        \"neg_hits\": neg_hits,\n",
    "                        \"source_file\": in_path.name,\n",
    "                    })\n",
    "                    hits_writer.writerow(hits_row)\n",
    "\n",
    "                processed += 1\n",
    "                if processed % PRINT_EVERY == 0:\n",
    "                    print(f\"  [{in_path.name}] processed {processed:,} lines\", flush=True)\n",
    "    finally:\n",
    "        per_file_fh.close()\n",
    "\n",
    "    print(f\"Done: wrote {processed:,} rows to {out_path.name}\")\n",
    "\n",
    "hits_fh.close()\n",
    "print(f\"\\nWrote combined positive/negative hits to {HITS_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositivityDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128,\n",
    "                 text_col=\"textOriginal\", label_col=\"sentiment\"):\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(csv_file, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "        if text_col not in df.columns or label_col not in df.columns:\n",
    "            raise ValueError(f\"Expected columns '{text_col}' and '{label_col}' in {csv_file}. \"\n",
    "                             f\"Found: {list(df.columns)}\")\n",
    "\n",
    "        mapping = {\"positive\": 1, \"negative\": 0}\n",
    "        df[\"label\"] = df[label_col].str.strip().str.lower().map(mapping)\n",
    "\n",
    "        df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "        df = df[(df[\"label\"].isin([0, 1])) & (df[text_col].str.strip() != \"\")]\n",
    "\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.float).unsqueeze(0)  # shape (1,)\n",
    "\n",
    "        return input_ids, attention_mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "all_df = pd.read_csv(\"positivity_hits_all.csv\", engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "all_df = all_df[[\"textOriginal\", \"sentiment\"]].dropna()\n",
    "train_df, test_df = train_test_split(\n",
    "    all_df, test_size=0.2, stratify=all_df[\"sentiment\"].str.strip().str.lower(), random_state=42\n",
    ")\n",
    "train_df.to_csv(\"positivity_hits_train.csv\", index=False)\n",
    "test_df.to_csv(\"positivity_hits_test.csv\", index=False)\n",
    "\n",
    "dataset_train = PositivityDataset(\"positivity_hits_train.csv\", tokenizer, max_length=max_length)\n",
    "dataset_test  = PositivityDataset(\"positivity_hits_test.csv\",  tokenizer, max_length=max_length)\n",
    "\n",
    "P_loader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "P_loader_test  = DataLoader(dataset_test,  batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositivityScorer(nn.Module):\n",
    "    def __init__(self, transformer, hidden_dim, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.transformer = transformer\n",
    "        transformer_output_dim = transformer.config.hidden_size\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = out.last_hidden_state[:, 0, :]  # CLS token\n",
    "        out = self.classifier(cls_token)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log = []\n",
    "        self.best_loss = np.inf\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train() \n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Step (1)        \n",
    "            for input_ids, attention_mask, labels in loader_train:\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                labels = labels.float().to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels) \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_loss / len(loader_train)\n",
    "            avg_test_loss = self.evaluate(loader_test)\n",
    "\n",
    "            self.loss_train_log.append(avg_train_loss)\n",
    "            self.loss_test_log.append(avg_test_loss)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epochs %d/%d' % (epoch+1, epochs))\n",
    "                print('Train Loss = %.4f' % avg_train_loss, end=', ')\n",
    "                print('Val Loss = %.4f' % avg_test_loss)\n",
    "\n",
    "        # Step (5) save the best model\n",
    "            if avg_test_loss < self.best_loss:\n",
    "                self.best_loss = avg_test_loss\n",
    "                self.best_epoch = epoch + 1\n",
    "                torch.save(self.state_dict(), 'positivity_best_params.pt')\n",
    "        \n",
    "        print(f'Best model saved at epoch {self.best_epoch} with loss {self.best_loss:.4f}.')\n",
    "\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.eval() \n",
    "        loss_fn = nn.BCELoss()\n",
    "        loss = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            total_loss = 0\n",
    "\n",
    "            for input_ids, attention_mask, labels in loader:\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "                labels = labels.float().to(self.device)\n",
    "\n",
    "                outputs = self.forward(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        loss = total_loss / len(loader)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        self.eval() \n",
    "\n",
    "        x_all, y_all, pred = [], [], [] \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in loader:\n",
    "                input_ids = input_ids.long().to(self.device)\n",
    "                attention_mask = attention_mask.long().to(self.device)\n",
    "\n",
    "                outputs = self.forward(input_ids, attention_mask)\n",
    "                preds = (outputs > 0.5).long().cpu().numpy()\n",
    "\n",
    "                x_all.extend(input_ids.cpu().numpy())\n",
    "                y_all.extend(labels.cpu().numpy())\n",
    "                pred.extend(preds)\n",
    "\n",
    "            return x_all, y_all, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_lr = 1e-3\n",
    "P_epochs = 10\n",
    "P_model = PositivityScorer(transformer = transformer_model, hidden_dim = transformer_model.config.hidden_dim)\n",
    "P_optimizer = torch.optim.Adam(P_model.parameters(), lr=lr)\n",
    "P_model.Train(P_epochs, P_optimizer, P_loader_train, P_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous Output: \n",
    "\n",
    "# Epochs 1/10\n",
    "# Train Loss = 0.1570, Val Loss = 0.1190\n",
    "# Epochs 2/10\n",
    "# Train Loss = 0.1254, Val Loss = 0.1245\n",
    "# Epochs 3/10\n",
    "# Train Loss = 0.1187, Val Loss = 0.1600\n",
    "# Epochs 4/10\n",
    "# Train Loss = 0.1159, Val Loss = 0.1237\n",
    "# Epochs 5/10\n",
    "# Train Loss = 0.1062, Val Loss = 0.1282\n",
    "# Epochs 6/10\n",
    "# Train Loss = 0.1038, Val Loss = 0.1084\n",
    "# Epochs 7/10\n",
    "# Train Loss = 0.1027, Val Loss = 0.1073\n",
    "# Epochs 8/10\n",
    "# Train Loss = 0.0989, Val Loss = 0.1084\n",
    "# Epochs 9/10\n",
    "# Train Loss = 0.0918, Val Loss = 0.1024\n",
    "# Epochs 10/10\n",
    "# Train Loss = 0.0887, Val Loss = 0.0976\n",
    "# Best model saved at epoch 10 with loss 0.0976."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"comments1_filtered_english.csv\",\n",
    "    \"comments2_filtered_english.csv\",\n",
    "    \"comments3_filtered_english.csv\",\n",
    "    \"comments4_filtered_english.csv\",\n",
    "    \"comments5_filtered_english.csv\",\n",
    "]\n",
    "N_PER_FILE   = 2000\n",
    "PRINT_EVERY  = 20\n",
    "MAX_LENGTH   = 128\n",
    "PRETRAINED   = \"distilbert-base-uncased\"\n",
    "BEST_WEIGHTS = \"positivity_best_params.pt\"\n",
    "OUTPUT_CSV   = \"positivity_scored_batch.csv\"\n",
    "\n",
    "TEXT_COL_CANDIDATES = [\"textOriginal\"]  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "transformer_model = AutoModel.from_pretrained(PRETRAINED)\n",
    "\n",
    "state = torch.load(BEST_WEIGHTS, map_location=\"cpu\")\n",
    "\n",
    "P_model.load_state_dict(state, strict=True)\n",
    "P_model.eval()\n",
    "\n",
    "\n",
    "def rescale_positivity(arr, lo=0.35):\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    # piecewise linear\n",
    "    below = a < lo\n",
    "    above = ~below\n",
    "    out = np.empty_like(a, dtype=float)\n",
    "    out[below] = (a[below] / lo) * 0.5\n",
    "    out[above] = 0.5 + ((a[above] - lo) / (1.0 - lo)) * 0.5\n",
    "    # numeric safety\n",
    "    np.clip(out, 0.0, 1.0, out=out)\n",
    "    return out\n",
    "\n",
    "def predict_scores(texts, batch_size=BATCH_SIZE):\n",
    "    scores = []\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        chunk = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            chunk,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        probs = model(input_ids, attention_mask)         \n",
    "        scores.extend(probs.detach().cpu().tolist())\n",
    "\n",
    "        done = min(i + batch_size, n)\n",
    "        if done % PRINT_EVERY == 0:\n",
    "            print(f\"processed {done:,}/{n:,}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for fpath in FILES:\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Warning: {fpath} not found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(fpath, engine=\"python\", on_bad_lines=\"skip\", dtype=str, encoding=\"utf-8\", nrows=N_PER_FILE)\n",
    "    if df.empty:\n",
    "        print(f\"{fpath}: no rows read; skipping.\")\n",
    "        continue\n",
    "\n",
    "    text_col = pick_text_col(df)\n",
    "    df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "    n = len(df)\n",
    "    block_scores = []\n",
    "    for start in range(0, n, PRINT_EVERY):\n",
    "        end = min(start + PRINT_EVERY, n)\n",
    "        texts_block = df[text_col].iloc[start:end].tolist()\n",
    "        block_scores.extend(predict_scores(texts_block))\n",
    "        print(f\"[{os.path.basename(fpath)}] processed {end}/{n}\")\n",
    "\n",
    "    raw = np.array(block_scores[:n], dtype=float)\n",
    "    scaled = rescale_positivity(raw, lo=0.35)\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out[\"positivity_score_raw\"] = raw.astype(float)\n",
    "    df_out[\"positivity_score\"] = scaled.astype(float)\n",
    "\n",
    "    # Keep threshold at 0.5 on the *scaled* score\n",
    "    df_out[\"predicted_sentiment\"] = (df_out[\"positivity_score\"] >= 0.5).map({True: \"positive\", False: \"negative\"})\n",
    "    df_out[\"source_file\"] = os.path.basename(fpath)\n",
    "\n",
    "    all_results.append(df_out)\n",
    "\n",
    "\n",
    "\n",
    "results = pd.concat(all_results, ignore_index=True)\n",
    "results[\"positivity_score_raw\"] = results[\"positivity_score_raw\"].map(lambda x: float(f\"{x:.6f}\"))\n",
    "results[\"positivity_score\"]     = results[\"positivity_score\"].map(lambda x: float(f\"{x:.6f}\"))\n",
    "\n",
    "results.to_csv(\"positivity_scored_batch.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved {len(results)} scored rows to positivity_scored_batch.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positivity was slightly skeweed, so I manually shifted it\n",
    "\n",
    "INPUT_CSV = \"positivity_scored_batch.csv\"\n",
    "PIVOT = 0.35\n",
    "\n",
    "\n",
    "def rescale_positivity(arr, lo=0.35):\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    out = np.empty_like(a, dtype=float)\n",
    "    below = a < lo\n",
    "    out[below] = (a[below] / lo) * 0.5\n",
    "    out[~below] = 0.5 + ((a[~below] - lo) / (1.0 - lo)) * 0.5\n",
    "    np.clip(out, 0.0, 1.0, out=out)\n",
    "    return out\n",
    "\n",
    "SCORE_CANDIDATES = [\"positivity_score_raw\", \"positivity_score\", \"quality_score\", \"quality_score_raw\"]\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "score_col = next((c for c in SCORE_CANDIDATES if c in df.columns), None)\n",
    "\n",
    "raw = pd.to_numeric(df[score_col], errors=\"coerce\").fillna(0.0).to_numpy(dtype=float)\n",
    "df[\"positivity_score_raw\"] = raw\n",
    "df[\"positivity_score\"] = rescale_positivity(raw, lo=PIVOT)\n",
    "\n",
    "df[\"predicted_sentiment\"] = np.where(df[\"positivity_score\"] >= 0.5, \"positive\", \"negative\")\n",
    "\n",
    "root, ext = os.path.splitext(INPUT_CSV)\n",
    "output_csv = f\"{root}_rescaled{ext}\"\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "pos = int((df[\"positivity_score\"] >= 0.5).sum())\n",
    "neg = len(df) - pos\n",
    "print(f\"Saved: {output_csv}  |  positives={pos:,}  negatives={neg:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance in terms of product interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [\n",
    "    f\"comments{i}_filtered_english_with_video_details_tagged_positivity_quality.csv\"\n",
    "    for i in (1, 2, 3, 4, 5)\n",
    "]\n",
    "\n",
    "LOREAL_BRANDS = {\n",
    "    \"l'oreal\": [r\"\\bl['’]?or[ée]al\\b\", r\"\\bloreal\\b\"],\n",
    "    \"l'oreal paris\": [r\"\\bl['’]?or[ée]al\\s+paris\\b\"],\n",
    "    \"garnier\": [r\"\\bgarnier\\b\"],\n",
    "    \"maybelline\": [r\"\\bmaybelline\\b\", r\"\\bmaybelline\\s+new\\s+york\\b\"],\n",
    "    \"nyx\": [r\"\\bnyx\\b\", r\"\\bnyx\\s+professional\\s+makeup\\b\"],\n",
    "    \"essie\": [r\"\\bessie\\b\"],\n",
    "    \"lancome\": [r\"\\blanc[ôo]me\\b\"],\n",
    "    \"ysl beauty\": [r\"\\b(yves\\s+saint\\s+laurent|ysl)\\b\", r\"\\bysl\\s+beauty\\b\"],\n",
    "    \"giorgio armani / armani beauty\": [r\"\\barmani\\b\", r\"\\bgiorgio\\s+armani\\b\", r\"\\barmani\\s+beauty\\b\"],\n",
    "    \"valentino beauty\": [r\"\\bvalentino\\b\"],\n",
    "    \"prada beauty\": [r\"\\bprada\\b\"],\n",
    "    \"mugler\": [r\"\\bmugler\\b\"],\n",
    "    \"azzaro\": [r\"\\bazzaro\\b\"],\n",
    "    \"maison margiela\": [r\"\\bmaison\\s+margiela\\b\", r\"\\bmargiela\\b\"],\n",
    "    \"ralph lauren fragrances\": [r\"\\bralph\\s+lauren\\b\"],\n",
    "    \"diesel fragrances\": [r\"\\bdiesel\\b\"],\n",
    "    \"cacharel\": [r\"\\bcacharel\\b\"],\n",
    "    \"urban decay\": [r\"\\burban\\s+decay\\b\"],\n",
    "    \"kiehl's\": [r\"\\bkiehl['’]s\\b\", r\"\\bkiehls\\b\"],\n",
    "    \"helena rubinstein\": [r\"\\bhelena\\s+rubinstein\\b\"],\n",
    "    \"it cosmetics\": [r\"\\bit\\s+cosmetics\\b\"],\n",
    "    \"biotherm\": [r\"\\bbiotherm\\b\"],\n",
    "    \"aesop\": [r\"\\baesop\\b\"],\n",
    "    \"la roche-posay\": [r\"\\bla\\s*roche[-\\s]*posay\\b\", r\"\\blrp\\b\"],\n",
    "    \"vichy\": [r\"\\bvichy\\b\"],\n",
    "    \"skinceuticals\": [r\"\\bskinceuticals\\b\"],\n",
    "    \"cerave\": [r\"\\bcer[aà]?\\s*ve\\b\", r\"\\bcerave\\b\"],\n",
    "    \"thayers\": [r\"\\bthayers\\b\"],\n",
    "    \"youth to the people\": [r\"\\byouth\\s+to\\s+the\\s+people\\b\"],\n",
    "    \"l'oreal professionnel\": [r\"\\bl['’]?or[ée]al\\s+professionnel\\b\"],\n",
    "    \"kerastase\": [r\"\\bk[ée]rastase\\b\", r\"\\bkerastase\\b\"],\n",
    "    \"redken\": [r\"\\bredken\\b\"],\n",
    "    \"matrix\": [r\"\\bmatrix\\b\"],\n",
    "    \"pureology\": [r\"\\bpureology\\b\"],\n",
    "    \"mizani\": [r\"\\bmizani\\b\"],\n",
    "    \"biolage\": [r\"\\bbiolage\\b\"],\n",
    "    \"shu uemura\": [r\"\\bshu\\s+uemura\\b\"],\n",
    "}\n",
    "\n",
    "OTHER_BEAUTY = {\n",
    "    \"fenty\": [r\"\\bfenty\\b\", r\"\\bfenty\\s+(beauty|skin)\\b\"],\n",
    "    \"glossier\": [r\"\\bglossier\\b\"],\n",
    "    \"nars\": [r\"\\bnars\\b\"],\n",
    "    \"huda beauty\": [r\"\\bhuda\\s+beauty\\b\", r\"\\bhuda\\b\"],\n",
    "    \"rare beauty\": [r\"\\brare\\s*beauty\\b\", r\"\\brarebeauty\\b\"],\n",
    "    \"charlotte tilbury\": [r\"\\bcharlotte\\s+tilbury\\b\"],\n",
    "    \"milk makeup\": [r\"\\bmilk\\s+makeup\\b\"],\n",
    "    \"olaplex\": [r\"\\bolaplex\\b\"],\n",
    "    \"drunk elephant\": [r\"\\bdrunk\\s+elephant\\b\"],\n",
    "    \"the ordinary\": [r\"\\bthe\\s+ordinary\\b\"],\n",
    "    \"kosas\": [r\"\\bkosas\\b\"],\n",
    "    \"clinique\": [r\"\\bclinique\\b\"],\n",
    "    \"estee lauder\": [r\"\\best[ée]e\\s+lauder\\b\", r\"\\bestee\\s+lauder\\b\"],\n",
    "    \"shiseido\": [r\"\\bshiseido\\b\"],\n",
    "    \"too faced\": [r\"\\btoo\\s+faced\\b\"],\n",
    "    \"tatcha\": [r\"\\btatcha\\b\"],\n",
    "    \"bobbi brown\": [r\"\\bbobbi\\s+brown\\b\"],\n",
    "    \"la mer\": [r\"\\bla\\s+mer\\b\"],\n",
    "    \"pat mcgrath\": [r\"\\bpat\\s+mcgrath\\b\"],\n",
    "    \"colourpop\": [r\"\\bcolour\\s*pop\\b\", r\"\\bcolourpop\\b\"],\n",
    "    \"morphe\": [r\"\\bmorphe\\b\"],\n",
    "    \"kvd beauty\": [r\"\\bkvd\\b\", r\"\\bkvd\\s+beauty\\b\"],\n",
    "    \"benefit\": [r\"\\bbenefit\\b\"],\n",
    "    \"tarte\": [r\"\\btarte\\b\"],\n",
    "    \"e.l.f.\": [r\"\\be\\.?l\\.?f\\.?\\b\", r\"\\belf\\b\"],\n",
    "    \"dior\": [r\"\\bdior\\b\"],\n",
    "    \"chanel\": [r\"\\bchanel\\b\"],\n",
    "    \"hourglass\": [r\"\\bhourglass\\b\"],\n",
    "    \"laura mercier\": [r\"\\blaura\\s+mercier\\b\"],\n",
    "    \"natasha denona\": [r\"\\bnatasha\\s+denona\\b\"],\n",
    "    \"skims\": [r\"\\bskims\\b\"],\n",
    "    \"dyson\": [r\"\\bdyson\\b\"],\n",
    "    \"ghd\": [r\"\\bghd\\b\"],\n",
    "    \"olay\": [r\"\\bolay\\b\"],\n",
    "    \"neutrogena\": [r\"\\bneutrogena\\b\"],\n",
    "    \"sephora\": [r\"\\bsephora\\b\"],\n",
    "    \"ulta\": [r\"\\bulta\\b\"],\n",
    "}\n",
    "\n",
    "LOREAL_PATTERNS = {k: [re.compile(p, re.I) for p in v] for k, v in LOREAL_BRANDS.items()}\n",
    "OTHER_PATTERNS  = {k: [re.compile(p, re.I) for p in v] for k, v in OTHER_BEAUTY.items()}\n",
    "\n",
    "INTENT_PATTERNS = [\n",
    "    r\"\\bhow\\s+much\\b\", r\"\\bprice\\b\", r\"\\bcost\\b\", r\"\\bmsrp\\b\", r\"[$£€]\\s*\\d\",\n",
    "    r\"\\brelease\\b\", r\"\\blaunch\\b\", r\"\\bcoming\\s+out\\b\", r\"\\bdrop\\b\", r\"\\bavailable\\b\",\n",
    "    r\"\\bback\\s+in\\s+stock\\b\", r\"\\brestock\\b\",\n",
    "    r\"\\bwhere\\s+(can\\s+)?(i\\s+)?(buy|get|purchase)\\b\",\n",
    "    r\"\\bwhere\\s+to\\s+(buy|get|purchase|cop)\\b\",\n",
    "    r\"\\bbuy\\b\", r\"\\bpurchase\\b\", r\"\\border\\b\",\n",
    "    r\"\\blink\\b\", r\"\\burl\\b\", r\"link\\s*(pls|please)?\",\n",
    "    r\"\\bdeliver(y|ies|ing)?\\b\", r\"\\bship(ping|s|ped)?\\b\",\n",
    "    r\"\\bavailable\\s+in\\b\",\n",
    "    r\"\\b(in|to)\\s+(US|USA|UK|Canada|Australia|India|Singapore|Malaysia|EU|Europe)\\b\",\n",
    "]\n",
    "INTENT_REGEXES = [re.compile(p, re.I) for p in INTENT_PATTERNS]\n",
    "\n",
    "COMMENT_COLS = [\"textOriginal\", \"comment_text\", \"text\"]\n",
    "def pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def norm_text(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return \"\"\n",
    "    return str(x).lower()\n",
    "\n",
    "def any_match(pattern_dict, text):\n",
    "    hits = []\n",
    "    for name, regs in pattern_dict.items():\n",
    "        if any(r.search(text) for r in regs):\n",
    "            hits.append(name)\n",
    "    return hits\n",
    "\n",
    "def has_intent(text):\n",
    "    return any(rx.search(text) for rx in INTENT_REGEXES)\n",
    "\n",
    "def out_name_from_in(path: str) > str:\n",
    "    base = Path(path).name\n",
    "    m = re.search(r\"comments(\\d+)\", base)\n",
    "    if m:\n",
    "        return f\"comments{m.group(1)}_filtered_all_comment_metrics.csv\"\n",
    "    return base.replace(\".csv\", \"_all_comment_metrics.csv\")\n",
    "\n",
    "\n",
    "for path in INPUT_FILES:\n",
    "    if not Path(path).exists():\n",
    "        print(f\"Missing: {path} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "    if df.empty:\n",
    "        print(f\"Empty file: {path}\")\n",
    "        continue\n",
    "\n",
    "    comment_col = pick_col(df, COMMENT_COLS)\n",
    "\n",
    "    text_lc = df[comment_col].fillna(\"\").astype(str).map(str).str.lower()\n",
    "\n",
    "    loreal_hits = text_lc.apply(lambda t: any_match(LOREAL_PATTERNS, t))\n",
    "    other_hits  = text_lc.apply(lambda t: any_match(OTHER_PATTERNS, t))\n",
    "    intent_hit  = text_lc.apply(lambda t: has_intent(t))\n",
    "\n",
    "    rel = []\n",
    "    rel_rule = []\n",
    "    for lhits, ohits, ih in zip(loreal_hits, other_hits, intent_hit):\n",
    "        if lhits:\n",
    "            rel.append(1.0)\n",
    "            rel_rule.append(\"loreal_brand\")\n",
    "        elif ohits:\n",
    "            rel.append(0.75)\n",
    "            rel_rule.append(\"other_beauty_brand\")\n",
    "        elif ih:\n",
    "            rel.append(0.5)\n",
    "            rel_rule.append(\"buy_intent\")\n",
    "        else:\n",
    "            rel.append(0.0)\n",
    "            rel_rule.append(\"none\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"relevance\"] = rel\n",
    "    out[\"relevance_rule\"] = rel_rule \n",
    "\n",
    "    out_path = out_name_from_in(path)\n",
    "    out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"{Path(path).name} → {out_path} | wrote {len(out):,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking comment replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = [f\"comments{i}_filtered_all_comment_metrics.csv\" for i in (1, 2, 3, 4, 5)]\n",
    "CHUNKSIZE = 10_000\n",
    "\n",
    "def dedup_preserve_order(lst):\n",
    "    seen, out = set(), []\n",
    "    for x in lst:\n",
    "        if x not in seen:\n",
    "            out.append(x)\n",
    "            seen.add(x)\n",
    "    return out\n",
    "\n",
    "def build_global_children_lookup(paths):\n",
    "    mapping = defaultdict(list)\n",
    "    for path in paths:\n",
    "        p = Path(path)\n",
    "        if not p.exists():\n",
    "            print(f\"Missing: {p.name} (skip in mapping)\")\n",
    "            continue\n",
    "\n",
    "        usecols = [\"commentId\", \"parentCommentId\"]\n",
    "        for ch in pd.read_csv(p, engine=\"python\", on_bad_lines=\"skip\", dtype=str,\n",
    "                              usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            if ch.empty:\n",
    "                continue\n",
    "            ch[\"commentId\"] = ch[\"commentId\"].fillna(\"\").astype(str).str.strip()\n",
    "            ch[\"parentCommentId\"] = ch[\"parentCommentId\"].fillna(\"\").astype(str).str.strip()\n",
    "            ch = ch[ch[\"parentCommentId\"] != \"\"]\n",
    "            if ch.empty:\n",
    "                continue\n",
    "            for parent, grp in ch.groupby(\"parentCommentId\")[\"commentId\"]:\n",
    "                mapping[parent].extend(grp.tolist())\n",
    "\n",
    "    for k, v in list(mapping.items()):\n",
    "        mapping[k] = dedup_preserve_order(v)\n",
    "\n",
    "    lookup_df = (\n",
    "        pd.DataFrame({\n",
    "            \"parentCommentId\": list(mapping.keys()),\n",
    "            \"num_replies\": [len(v) for v in mapping.values()],\n",
    "            \"reply_comment_ids\": [\",\".join(v) for v in mapping.values()],\n",
    "        })\n",
    "        .sort_values(\"num_replies\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return mapping, lookup_df\n",
    "\n",
    "def enrich_with_global_mapping(path, mapping, out_suffix=\"_with_replies_global.csv\"):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"Missing: {p.name} (skip enrich)\")\n",
    "        return\n",
    "\n",
    "    out_path = p.with_name(p.stem + out_suffix)\n",
    "    wrote_header = False\n",
    "    orig_cols_order = None\n",
    "\n",
    "    for ch in pd.read_csv(p, engine=\"python\", on_bad_lines=\"skip\", dtype=str, chunksize=CHUNKSIZE):\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        if orig_cols_order is None:\n",
    "            orig_cols_order = list(ch.columns)\n",
    "\n",
    "        ch[\"commentId\"] = ch[\"commentId\"].fillna(\"\").astype(str).str.strip()\n",
    "        ch[\"num_replies\"] = ch[\"commentId\"].map(lambda cid: len(mapping.get(cid, []))).astype(int)\n",
    "        ch[\"reply_comment_ids\"] = ch[\"commentId\"].map(lambda cid: \" | \".join(mapping.get(cid, [])))\n",
    "\n",
    "        new_cols = [\"num_replies\", \"reply_comment_ids\"]\n",
    "        ch = ch.reindex(columns=orig_cols_order + [c for c in new_cols if c not in orig_cols_order])\n",
    "\n",
    "        ch.to_csv(out_path, mode=(\"a\" if wrote_header else \"w\"),\n",
    "                  header=(not wrote_header), index=False, encoding=\"utf-8\")\n",
    "        wrote_header = True\n",
    "\n",
    "    print(f\"{p.name} → {out_path.name}\")\n",
    "\n",
    "global_map, global_lookup = build_global_children_lookup(INPUTS)\n",
    "global_lookup.to_csv(\"comments_global_reply_lookup.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote global lookup: comments_global_reply_lookup.csv ({len(global_lookup):,} parents)\")\n",
    "\n",
    "for path in INPUTS:\n",
    "    enrich_with_global_mapping(path, global_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TPL = \"comments{idx}_filtered_all_comment_metrics_with_replies_global.csv\"\n",
    "OUTPUT_TPL = \"comments{idx}_scored.csv\"\n",
    "INDEXES = [1, 2, 3, 4, 5]\n",
    "CHUNKSIZE = 5000  \n",
    "ROUND = 6             \n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def coerce_int(s):\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    x = x.fillna(0).astype(float)  \n",
    "    return x.astype(int)\n",
    "\n",
    "def coerce_float(s, default=1.0):\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    x = x.fillna(default)\n",
    "    return x.astype(float)\n",
    "\n",
    "def parse_dt(s):\n",
    "    return pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "\n",
    "def like_score(likes: pd.Series) > pd.Series:\n",
    "    s = pd.Series(0.0, index=likes.index, dtype=float)\n",
    "    s = s.mask((likes >= 20)   & (likes <= 99),   0.025)\n",
    "    s = s.mask((likes >= 100)  & (likes <= 499),  0.050)\n",
    "    s = s.mask((likes >= 500)  & (likes <= 2499), 0.075)\n",
    "    s = s.mask((likes >= 2500),                  0.100)\n",
    "    return s.fillna(0.0)\n",
    "\n",
    "def published_updated_score(pub_dt: pd.Series, upd_dt: pd.Series) > pd.Series:\n",
    "    diffs = (pub_dt.notna() & upd_dt.notna() & (pub_dt != upd_dt))\n",
    "    return diffs.astype(float) * 0.08\n",
    "\n",
    "def video_comment_window_score(video_dt: pd.Series, cmt_dt: pd.Series) > pd.Series:\n",
    "    delta = (cmt_dt - video_dt)\n",
    "    cond = video_dt.notna() & cmt_dt.notna() & (delta.dt.total_seconds() >= 0) & (delta.dt.total_seconds() <= 48 * 3600)\n",
    "    return cond.astype(float) * 0.07\n",
    "\n",
    "def relevance_add_score(rel: pd.Series) > pd.Series:\n",
    "    r = coerce_float(rel, default=np.nan).round(2)\n",
    "    out = pd.Series(0.0, index=r.index, dtype=float)\n",
    "    out = out.mask(r == 1.00, 0.15)\n",
    "    out = out.mask(r == 0.75, 0.15)\n",
    "    out = out.mask(r == 0.50, 0.10)\n",
    "    return out.fillna(0.0)\n",
    "\n",
    "def replies_score(nrep: pd.Series) > pd.Series:\n",
    "    n = coerce_int(nrep)\n",
    "    s = pd.Series(0.0, index=n.index, dtype=float)\n",
    "    s = s.mask((n >= 1) & (n <= 2), 0.05)\n",
    "    s = s.mask((n >= 3) & (n <= 4), 0.075)\n",
    "    s = s.mask((n >= 5),           0.10)\n",
    "    return s.fillna(0.0)\n",
    "\n",
    "def ensure_col(df, wanted): \n",
    "    if getattr(df, \"_stripped_cols_done\", False) is False:\n",
    "        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "        df._stripped_cols_done = True\n",
    "    return wanted if wanted in df.columns else None\n",
    "\n",
    "def pick_video_published_col(df):\n",
    "    \n",
    "    for name in [\"video_publishedAt\", \"videoPublishedAt\", \"publishedAt_video\", \"video_publishedAt_utc\"]:\n",
    "        if ensure_col(df, name):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "for idx in INDEXES:\n",
    "    in_path  = INPUT_TPL.format(idx=idx)\n",
    "    out_path = OUTPUT_TPL.format(idx=idx)\n",
    "\n",
    "    if not Path(in_path).exists():\n",
    "        print(f\"Skip (missing): {in_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nScoring {in_path} → {out_path}\")\n",
    "\n",
    "    wrote_header = False\n",
    "    orig_cols_order = None\n",
    "\n",
    "    for chunk in pd.read_csv(in_path, engine=\"python\", on_bad_lines=\"skip\", dtype=str, chunksize=CHUNKSIZE):\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        chunk.columns = [c.strip() if isinstance(c, str) else c for c in chunk.columns]\n",
    "        if orig_cols_order is None:\n",
    "            orig_cols_order = list(chunk.columns)\n",
    "\n",
    "        like_col  = ensure_col(chunk, \"likeCount\")\n",
    "        pub_col   = ensure_col(chunk, \"publishedAt\")\n",
    "        upd_col   = ensure_col(chunk, \"updatedAt\")\n",
    "        nrep_col  = ensure_col(chunk, \"num_replies\")\n",
    "        rel_col   = ensure_col(chunk, \"relevance\")\n",
    "        q_col     = ensure_col(chunk, \"quality_score\")       # multiplier\n",
    "        p_col     = ensure_col(chunk, \"positivity_score\")    # multiplier\n",
    "\n",
    "        vid_pub_col = pick_video_published_col(chunk)\n",
    "\n",
    "        # Base score\n",
    "        base = pd.Series(0.5, index=chunk.index, dtype=float)\n",
    "\n",
    "        # likeCountScore\n",
    "        likes = coerce_int(chunk[like_col]) if like_col else pd.Series(0, index=chunk.index, dtype=int)\n",
    "        likeScore = like_score(likes)\n",
    "\n",
    "        # publishedVsUpdatedScore\n",
    "        if pub_col and upd_col:\n",
    "            pub_dt = parse_dt(chunk[pub_col])\n",
    "            upd_dt = parse_dt(chunk[upd_col])\n",
    "            puScore = published_updated_score(pub_dt, upd_dt)\n",
    "        else:\n",
    "            puScore = pd.Series(0.0, index=chunk.index, dtype=float)\n",
    "\n",
    "        # videoPublishedCommentPublishedScore\n",
    "        if vid_pub_col and pub_col:\n",
    "            vid_dt = parse_dt(chunk[vid_pub_col])\n",
    "            cmt_dt = parse_dt(chunk[pub_col])\n",
    "            vcScore = video_comment_window_score(vid_dt, cmt_dt)\n",
    "        else:\n",
    "            vcScore = pd.Series(0.0, index=chunk.index, dtype=float)\n",
    "\n",
    "        # relevanceScore\n",
    "        relScore = relevance_add_score(chunk[rel_col]) if rel_col else pd.Series(0.0, index=chunk.index, dtype=float)\n",
    "\n",
    "        # replyCountScore\n",
    "        repScore = replies_score(chunk[nrep_col]) if nrep_col else pd.Series(0.0, index=chunk.index, dtype=float)\n",
    "\n",
    "        # Sum additive parts\n",
    "        intermediate = base + likeScore + puScore + vcScore + relScore + repScore\n",
    "\n",
    "        # Multipliers\n",
    "        q_mult = coerce_float(chunk[q_col], default=1.0) if q_col else pd.Series(1.0, index=chunk.index, dtype=float)\n",
    "        p_mult = coerce_float(chunk[p_col], default=1.0) if p_col else pd.Series(1.0, index=chunk.index, dtype=float)\n",
    "\n",
    "        final = intermediate * q_mult * p_mult\n",
    "\n",
    "        out = chunk.copy()\n",
    "        out[\"likeCountScore\"]                       = likeScore.round(ROUND)\n",
    "        out[\"publishedVsUpdatedScore\"]              = puScore.round(ROUND)\n",
    "        out[\"videoPublishedCommentPublishedScore\"]  = vcScore.round(ROUND)\n",
    "        out[\"relevanceScore\"]                       = relScore.round(ROUND)\n",
    "        out[\"replyCountScore\"]                      = repScore.round(ROUND)\n",
    "        out[\"intermediateScore\"]                    = intermediate.round(ROUND)\n",
    "        out[\"commentScorefinal\"]                    = final.round(ROUND)\n",
    "\n",
    "        new_cols = [\n",
    "            \"likeCountScore\",\n",
    "            \"publishedVsUpdatedScore\",\n",
    "            \"videoPublishedCommentPublishedScore\",\n",
    "            \"relevanceScore\",\n",
    "            \"replyCountScore\",\n",
    "            \"intermediateScore\",\n",
    "            \"commentScorefinal\",\n",
    "        ]\n",
    "        out = out.reindex(columns=orig_cols_order + [c for c in new_cols if c not in orig_cols_order])\n",
    "\n",
    "        out.to_csv(out_path, mode=(\"a\" if wrote_header else \"w\"),\n",
    "                   header=(not wrote_header), index=False, encoding=\"utf-8\")\n",
    "        wrote_header = True\n",
    "\n",
    "    print(f\"Done → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINE_VIDS = [\n",
    "    \"videos_tagged_algo.csv\",\n",
    "    \"videos_tagged_keywords.csv\",\n",
    "    \"leftover_videos_tagged.csv\",\n",
    "]\n",
    "\n",
    "NORMAL_COMMENT_LOOKUP = [f\"comments{i}_scored.csv\" for i in (1, 2, 3, 4, 5)]\n",
    "\n",
    "SPAM_COMMENT_LOOKUP = [\n",
    "    \"Quality_scoring/Base_datasets/emoji_spam_comments.csv\",\n",
    "    \"Quality_scoring/Base_datasets/keywords_spam.csv\",\n",
    "    \"Quality_scoring/Base_datasets/link_spam_comments.csv\",\n",
    "]\n",
    "\n",
    "VIDEO_ID_COL = \"videoId\"\n",
    "SCORE_COL    = \"commentScorefinal\"\n",
    "\n",
    "NEG_THRESH    = 0.30   \n",
    "BOOST_THRESH  = 0.60\n",
    "\n",
    "CHUNKSIZE_COMMENTS = 200_000\n",
    "CHUNKSIZE_SPAM     = 200_000\n",
    "\n",
    "OUT_PATH = \"videos_with_comment_metrics.csv\"\n",
    "ROUND = 6\n",
    "\n",
    "def coerce_float(x):\n",
    "    v = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return v.astype(float)\n",
    "\n",
    "def safe_str(s):\n",
    "    return \"\" if s is None or (isinstance(s, float) and math.isnan(s)) else str(s)\n",
    "\n",
    "\n",
    "\n",
    "vid_frames = []\n",
    "for path in COMBINE_VIDS:\n",
    "    if not Path(path).exists():\n",
    "        print(f\"Warning: missing {path} (skipped)\")\n",
    "        continue\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "    vid_frames.append(df)\n",
    "\n",
    "videos = pd.concat(vid_frames, ignore_index=True, sort=False)\n",
    "videos.columns = [c.strip() if isinstance(c, str) else c for c in videos.columns]\n",
    "videos = videos.drop_duplicates(subset=[VIDEO_ID_COL], keep=\"first\").reset_index(drop=True)\n",
    "print(f\"Combined videos: {len(videos):,} unique by {VIDEO_ID_COL}\")\n",
    "\n",
    "\n",
    "agg = {\n",
    "    \"count_normal\": defaultdict(int),\n",
    "    \"sum_scores\": defaultdict(float),\n",
    "    \"min_score\": defaultdict(lambda: float(\"inf\")),\n",
    "    \"max_score\": defaultdict(lambda: float(\"-inf\")),\n",
    "    \"count_lt_neg\": defaultdict(int),\n",
    "    \"count_ge_boost\": defaultdict(int),\n",
    "    \"scores_pipe\": defaultdict(list),  # may be large\n",
    "}\n",
    "\n",
    "for cpath in NORMAL_COMMENT_LOOKUP:\n",
    "    p = Path(cpath)\n",
    "    if not p.exists():\n",
    "        print(f\"Warning: missing {cpath} (skipped)\")\n",
    "        continue\n",
    "    print(f\"Scanning normal comments: {p.name}\")\n",
    "    for chunk in pd.read_csv(p, engine=\"python\", on_bad_lines=\"skip\",\n",
    "                             dtype=str, chunksize=CHUNKSIZE_COMMENTS):\n",
    "        if chunk.empty: \n",
    "            continue\n",
    "        cols = [c.strip() if isinstance(c, str) else c for c in chunk.columns]\n",
    "        chunk.columns = cols\n",
    "\n",
    "        vids  = chunk[VIDEO_ID_COL].astype(str)\n",
    "        scores = coerce_float(chunk[SCORE_COL])\n",
    "\n",
    "        mask = vids.str.strip().ne(\"\") & scores.notna()\n",
    "        vids = vids[mask]; scores = scores[mask]\n",
    "\n",
    "        for v, s in zip(vids.tolist(), scores.tolist()):\n",
    "            agg[\"count_normal\"][v] += 1\n",
    "            agg[\"sum_scores\"][v] += s\n",
    "            if s < agg[\"min_score\"][v]:\n",
    "                agg[\"min_score\"][v] = s\n",
    "            if s > agg[\"max_score\"][v]:\n",
    "                agg[\"max_score\"][v] = s\n",
    "            if s < NEG_THRESH:\n",
    "                agg[\"count_lt_neg\"][v] += 1\n",
    "            if s >= BOOST_THRESH:  \n",
    "                agg[\"count_ge_boost\"][v] += 1\n",
    "            agg[\"scores_pipe\"][v].append(f\"{s:.{ROUND}f}\")\n",
    "\n",
    "\n",
    "\n",
    "spam_counts = defaultdict(int)\n",
    "seen_spam_comments = set()  \n",
    "\n",
    "for spath in SPAM_COMMENT_LOOKUP:\n",
    "    p = Path(spath)\n",
    "    if not p.exists():\n",
    "        print(f\"Warning: missing spam file {spath} (skipped)\")\n",
    "        continue\n",
    "    print(f\"Scanning spam: {p.name}\")\n",
    "    for chunk in pd.read_csv(p, engine=\"python\", on_bad_lines=\"skip\",\n",
    "                             dtype=str, chunksize=CHUNKSIZE_SPAM):\n",
    "        if chunk.empty: \n",
    "            continue\n",
    "        chunk.columns = [c.strip() if isinstance(c, str) else c for c in chunk.columns]\n",
    "\n",
    "        if VIDEO_ID_COL not in chunk.columns:\n",
    "            missing = set([\"videoId\"]) - set(chunk.columns)\n",
    "            print(f\"  {p.name}: missing {missing}; skipping rows without videoId.\")\n",
    "            continue\n",
    "\n",
    "        if \"commentId\" in chunk.columns:\n",
    "            chunk[\"commentId\"] = chunk[\"commentId\"].astype(str).str.strip()\n",
    "            chunk = chunk[chunk[\"commentId\"].str.strip().ne(\"\")]\n",
    "            for cid, vid in zip(chunk[\"commentId\"], chunk[VIDEO_ID_COL].astype(str)):\n",
    "                key = (\"spam\", cid)\n",
    "                if key in seen_spam_comments:\n",
    "                    continue\n",
    "                seen_spam_comments.add(key)\n",
    "                if vid.strip():\n",
    "                    spam_counts[vid] += 1\n",
    "        else:\n",
    "            counts = chunk[VIDEO_ID_COL].astype(str).str.strip().value_counts()\n",
    "            for vid, n in counts.items():\n",
    "                if vid:\n",
    "                    spam_counts[vid] += int(n)\n",
    "\n",
    "\n",
    "\n",
    "def get_norm(v):\n",
    "    return agg[\"count_normal\"].get(v, 0)\n",
    "\n",
    "def get_spam(v):\n",
    "    return spam_counts.get(v, 0)\n",
    "\n",
    "def get_total(v):\n",
    "    return get_norm(v) + get_spam(v)\n",
    "\n",
    "def get_ratio(v):\n",
    "    tot = get_total(v)\n",
    "    return (get_spam(v) / tot) if tot > 0 else 0.0\n",
    "\n",
    "def mean_score(v):\n",
    "    n = get_norm(v)\n",
    "    return (agg[\"sum_scores\"].get(v, 0.0) / n) if n > 0 else np.nan\n",
    "\n",
    "def min_score(v):\n",
    "    m = agg[\"min_score\"].get(v, float(\"inf\"))\n",
    "    return (np.nan if m == float(\"inf\") else m)\n",
    "\n",
    "def max_score(v):\n",
    "    m = agg[\"max_score\"].get(v, float(\"-inf\"))\n",
    "    return (np.nan if m == float(\"-inf\") else m)\n",
    "\n",
    "def count_lt_neg(v):\n",
    "    return agg[\"count_lt_neg\"].get(v, 0)\n",
    "\n",
    "def count_ge_boost(v):\n",
    "    return agg[\"count_ge_boost\"].get(v, 0)\n",
    "\n",
    "# Compute columns\n",
    "vids = videos[VIDEO_ID_COL].astype(str)\n",
    "videos_out = videos.copy()\n",
    "\n",
    "videos_out[\"normal_comment_count\"] = vids.map(get_norm).astype(int)\n",
    "videos_out[\"spam_comment_count\"]   = vids.map(get_spam).astype(int)\n",
    "videos_out[\"total_comment_count\"]  = vids.map(get_total).astype(int)\n",
    "\n",
    "videos_out[\"spam_ratio\"]     = vids.map(get_ratio).astype(float).round(ROUND)\n",
    "videos_out[\"spam_ratio_pct\"] = (videos_out[\"spam_ratio\"] * 100.0).round(2)\n",
    "\n",
    "videos_out[\"comment_score_mean\"] = vids.map(mean_score).round(ROUND)\n",
    "videos_out[\"comment_score_min\"]  = vids.map(min_score).round(ROUND)\n",
    "videos_out[\"comment_score_max\"]  = vids.map(max_score).round(ROUND)\n",
    "\n",
    "videos_out[\"num_comments_lt_0_3\"]   = vids.map(count_lt_neg).astype(int)\n",
    "videos_out[\"num_comments_ge_0_6\"]   = vids.map(count_ge_boost).astype(int) \n",
    "\n",
    "\n",
    "videos_out.to_csv(OUT_PATH, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(videos_out):,} rows to {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT  = \"videos_with_comment_metrics.csv\"\n",
    "OUTPUT = \"videos_scored.csv\"\n",
    "\n",
    "\n",
    "\n",
    "def _to_int(s, default=0):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").fillna(default).astype(np.int64)\n",
    "\n",
    "def _to_float(s, default=np.nan):\n",
    "    out = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    return out if not np.isnan(default) else out\n",
    "\n",
    "\n",
    "\n",
    "def base_from_views_vec(views: pd.Series) > pd.Series:\n",
    "    v = views.values\n",
    "    base = np.where(v < 10_000,       0.45,\n",
    "            np.where(v < 100_000,     0.48,\n",
    "            np.where(v < 1_000_000,   0.52, 0.55)))\n",
    "    return pd.Series(base, index=views.index)\n",
    "\n",
    "\n",
    "def like_add_and_bucket(like_ratio: pd.Series):\n",
    "    lr = like_ratio.values\n",
    "    bucket = np.full(lr.shape, \">=5.00%\", dtype=object)\n",
    "    add    = np.full(lr.shape, 0.25, dtype=float)\n",
    "\n",
    "    mask3 = (lr < 0.0499)\n",
    "    bucket[mask3] = \"2.50 - 4.99%\"\n",
    "    add[mask3]    = 0.20\n",
    "\n",
    "    mask2 = (lr < 0.0249)\n",
    "    bucket[mask2] = \"1.0 - 2.49%\"\n",
    "    add[mask2]    = 0.15\n",
    "\n",
    "    mask1 = (lr < 0.0099)\n",
    "    bucket[mask1] = \"<0.99%\"\n",
    "    add[mask1]    = -0.15\n",
    "\n",
    "    return pd.Series(add, index=like_ratio.index), pd.Series(bucket, index=like_ratio.index)\n",
    "\n",
    "def comment_add_and_bucket(comment_ratio: pd.Series):\n",
    "    cr = comment_ratio.values\n",
    "    bucket = np.full(cr.shape, \">=0.05%\", dtype=object)\n",
    "    add    = np.full(cr.shape, 0.20, dtype=float)\n",
    "\n",
    "    mask2 = (cr < 0.00049)\n",
    "    bucket[mask2] = \"0.025 - 0.049%\"\n",
    "    add[mask2]    = 0.15\n",
    "\n",
    "    mask1 = (cr < 0.000024)\n",
    "    bucket[mask1] = \"<0.024%\"\n",
    "    add[mask1]    = -0.10\n",
    "\n",
    "    return pd.Series(add, index=comment_ratio.index), pd.Series(bucket, index=comment_ratio.index)\n",
    "\n",
    "def spam_penalty_and_reason(spam_ratio: pd.Series):\n",
    "    sr = spam_ratio.values\n",
    "    pen = np.zeros_like(sr, dtype=float)\n",
    "    reason = np.where(sr > 0.25, \"over 25%\", \"≤ 25% (none)\")\n",
    "    # slope 0.20 beyond 25%, cap at -0.15\n",
    "    over = np.maximum(0.0, sr - 0.25)\n",
    "    pen = -np.minimum(0.15, 0.20 * over)\n",
    "    return pd.Series(pen, index=spam_ratio.index), pd.Series(reason, index=spam_ratio.index)\n",
    "\n",
    "\n",
    "\n",
    "def mean_multiplier_asym_series(m: pd.Series, n: pd.Series, K=80, min_mult=0.50, max_mult=1.15):\n",
    "    n_val = n.values.astype(float)\n",
    "    m_val = m.values.astype(float)\n",
    "    mult = np.where((n_val <= 0) | np.isnan(m_val), 0.25, np.nan)\n",
    "\n",
    "    mask = (n_val > 0) & ~np.isnan(m_val)\n",
    "    shrink = np.zeros_like(n_val)\n",
    "    shrink[mask] = np.sqrt(n_val[mask] / (n_val[mask] + K))\n",
    "    m_adj = np.full_like(m_val, np.nan, dtype=float)\n",
    "    m_adj[mask] = 0.5 + (m_val[mask] - 0.5) * shrink[mask]\n",
    "\n",
    "    mult_raw = np.ones_like(m_val, dtype=float)\n",
    "    abv = mask & (m_adj >= 0.5)\n",
    "    mult_raw[abv] = 1.0 + 0.15 * ((m_adj[abv] - 0.5) / 0.5)\n",
    "    bel = mask & (m_adj < 0.5)\n",
    "    mult_raw[bel] = 1.0 - 0.50 * ((0.5 - m_adj[bel]) / 0.5)\n",
    "\n",
    "    vol_gate = np.minimum(1.0, np.sqrt(np.maximum(n_val, 0.0)) / 20.0)\n",
    "    mult_calc = 1.0 + (mult_raw - 1.0) * vol_gate\n",
    "    mult[mask] = np.clip(mult_calc[mask], min_mult, max_mult)\n",
    "\n",
    "    return (pd.Series(mult, index=m.index),\n",
    "            pd.Series(shrink, index=m.index),\n",
    "            pd.Series(m_adj, index=m.index),\n",
    "            pd.Series(vol_gate, index=m.index))\n",
    "\n",
    "\n",
    "def balance_multiplier_asym_series(g: pd.Series, b: pd.Series):\n",
    "    gi = g.values.astype(float); bi = b.values.astype(float)\n",
    "    R = (gi + 1.0) / (bi + 1.0)\n",
    "    t = np.tanh(np.log(R))# [-1, 1]\n",
    "    w = np.where(t >= 0, 0.06, 0.12) # heavier negative side\n",
    "    mult = 1.0 + w * t\n",
    "    return pd.Series(mult, index=g.index), pd.Series(R, index=g.index), pd.Series(t, index=g.index)\n",
    "\n",
    "\n",
    "\n",
    "def bad_mass_penalty_series(bad_count: pd.Series, normal_count: pd.Series, max_penalty=0.15):\n",
    "    n = normal_count.values.astype(float)\n",
    "    b = bad_count.values.astype(float)\n",
    "    frac_bad = np.divide(b, np.maximum(n, 1.0))\n",
    "    raw = 0.25 * frac_bad\n",
    "    vol_gate = np.minimum(1.0, n / 200.0)  \n",
    "    pen = -np.minimum(max_penalty, raw * vol_gate)\n",
    "    return pd.Series(pen, index=bad_count.index), pd.Series(frac_bad, index=bad_count.index), pd.Series(vol_gate, index=bad_count.index)\n",
    "\n",
    "df = pd.read_csv(INPUT, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "\n",
    "\n",
    "views   = _to_int(df.get(\"viewCount\", 0))\n",
    "likes   = _to_int(df.get(\"likeCount\", 0))\n",
    "c_rep   = _to_int(df.get(\"commentCount\", 0))\n",
    "n_norm  = _to_int(df.get(\"normal_comment_count\", 0))\n",
    "n_spam  = _to_int(df.get(\"spam_comment_count\", 0))\n",
    "n_total = _to_int(df.get(\"total_comment_count\", n_norm + n_spam)) \n",
    "\n",
    "comments_used = pd.Series(np.maximum.reduce([c_rep.values, n_total.values, (n_norm + n_spam).values]),\n",
    "                          index=df.index)\n",
    "comments_used_src = np.where(comments_used.values == c_rep.values,   \"commentCount(meta)\",\n",
    "                      np.where(comments_used.values == n_total.values, \"total_comment_count(observed)\",\n",
    "                               \"normal+spam(observed)\"))\n",
    "\n",
    "\n",
    "like_ratio    = likes / np.maximum(views, 1)\n",
    "comment_ratio = comments_used / np.maximum(views, 1)\n",
    "\n",
    "if \"spam_ratio\" in df.columns:\n",
    "    spam_ratio = _to_float(df[\"spam_ratio\"])\n",
    "    missing = spam_ratio.isna()\n",
    "    if missing.any():\n",
    "        spam_ratio.loc[missing] = (n_spam[missing] / np.maximum(comments_used[missing], 1)).astype(float)\n",
    "else:\n",
    "    spam_ratio = (n_spam / np.maximum(comments_used, 1)).astype(float)\n",
    "\n",
    "base = base_from_views_vec(views)\n",
    "\n",
    "like_add, like_bucket = like_add_and_bucket(like_ratio)\n",
    "cmt_add,  cmt_bucket  = comment_add_and_bucket(comment_ratio)\n",
    "sp_pen,   sp_reason   = spam_penalty_and_reason(spam_ratio)\n",
    "\n",
    "m_mean = _to_float(df.get(\"comment_score_mean\", np.nan))\n",
    "g_cnt  = _to_int(df.get(\"num_comments_ge_0_6\", 0))\n",
    "b_cnt  = _to_int(df.get(\"num_comments_lt_0_3\", 0))\n",
    "\n",
    "mean_mult, shrink_val, m_adj, mean_vol_gate = mean_multiplier_asym_series(m_mean, n_norm)\n",
    "bal_mult,  gb_ratio, tanh_lnR               = balance_multiplier_asym_series(g_cnt, b_cnt)\n",
    "bad_pen,   frac_bad, bad_vol_gate           = bad_mass_penalty_series(b_cnt, n_norm)\n",
    "\n",
    "pre = base + like_add + cmt_add + sp_pen + bad_pen\n",
    "pre_clamped = np.clip(pre, 0.0, 1.0)\n",
    "final = pre_clamped * mean_mult * bal_mult\n",
    "final = np.clip(final, 0.0, 1.0)\n",
    "\n",
    "out = df.copy()\n",
    "\n",
    "\n",
    "out[\"like_ratio\"]                = like_ratio.round(6)\n",
    "out[\"like_ratio_bucket\"]         = like_bucket\n",
    "\n",
    "out[\"comment_ratio\"]             = comment_ratio.round(6)\n",
    "out[\"comment_ratio_bucket\"]      = cmt_bucket\n",
    "\n",
    "out[\"comments_balancer_mean\"]           = mean_mult.round(6)\n",
    "\n",
    "out[\"comments_balancer_over_0.6\"]            = bal_mult.round(6)\n",
    "out[\"video_score\"]             = final.round(6)\n",
    "\n",
    "orig_cols = list(df.columns)\n",
    "new_cols = [c for c in out.columns if c not in orig_cols]\n",
    "out = out[orig_cols + new_cols]\n",
    "\n",
    "out.to_csv(OUTPUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(out):,} rows → {OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creator Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding all the unique creators in the system \n",
    "INPUT  = \"videos_scored.csv\"\n",
    "OUTPUT = \"unique_video_authors.csv\"\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def to_int(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "\n",
    "def to_float(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "\n",
    "df = pd.read_csv(INPUT, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "\n",
    "author_col = pick_col(df, [\"authorId\", \"channelId\", \"channel_id\"])\n",
    "\n",
    "video_id_col = pick_col(df, [\"videoId\", \"video_id\"])\n",
    "\n",
    "cat_col = pick_col(df, [\"defining_category\", \"defining category\"])\n",
    "if cat_col is None:\n",
    "    df[\"__no_category__\"] = \"\"\n",
    "    cat_col = \"__no_category__\"\n",
    "\n",
    "views  = to_int(df.get(\"viewCount\", 0))\n",
    "likes  = to_int(df.get(\"likeCount\", 0))\n",
    "if \"like_ratio\" in df.columns:\n",
    "    like_ratio = to_float(df[\"like_ratio\"]).fillna(0.0)\n",
    "else:\n",
    "    like_ratio = (likes / np.maximum(views, 1)).astype(float)\n",
    "\n",
    "if \"comment_ratio\" in df.columns:\n",
    "    comment_ratio = to_float(df[\"comment_ratio\"]).fillna(0.0)\n",
    "else:\n",
    "    comments = to_int(df.get(\"commentCount\", 0))\n",
    "    comment_ratio = (comments / np.maximum(views, 1)).astype(float)\n",
    "\n",
    "if \"spam_ratio\" in df.columns:\n",
    "    spam_ratio = to_float(df[\"spam_ratio\"]).fillna(0.0)\n",
    "elif \"spam_ratio_pct\" in df.columns:\n",
    "    spam_ratio = (to_float(df[\"spam_ratio_pct\"]) / 100.0).fillna(0.0)\n",
    "else:\n",
    "    n_spam = to_int(df.get(\"spam_comment_count\", 0))\n",
    "    n_tot  = to_int(df.get(\"total_comment_count\", 0))\n",
    "    spam_ratio = (n_spam / np.maximum(n_tot, 1)).astype(float)\n",
    "\n",
    "video_score = to_float(df[\"video_score\"]).fillna(0.0)\n",
    "\n",
    "df[\"_views\"]        = views\n",
    "df[\"_like_ratio\"]   = like_ratio\n",
    "df[\"_comment_ratio\"]= comment_ratio\n",
    "df[\"_spam_ratio\"]   = spam_ratio\n",
    "df[\"_video_score\"]  = video_score\n",
    "\n",
    "def resolve_main_category(group: pd.DataFrame) > str:\n",
    "    counts = group[cat_col].fillna(\"\").value_counts()\n",
    "    if counts.empty or counts.index[0] == \"\":\n",
    "        return \"\"\n",
    "    top_count = counts.max()\n",
    "    tied = counts[counts == top_count].index.tolist()\n",
    "    if len(tied) == 1:\n",
    "        return tied[0]\n",
    "    sub = group[group[cat_col].isin(tied)]\n",
    "    by_views = (\n",
    "        sub.groupby(cat_col, dropna=False)[\"_views\"]\n",
    "           .sum()\n",
    "           .sort_values(ascending=False)\n",
    "    )\n",
    "    return by_views.index[0] if len(by_views) else \"\"\n",
    "\n",
    "def join_videos(group: pd.DataFrame) > str:\n",
    "    vids = group.sort_values(\"_views\", ascending=False)[video_id_col].astype(str)\n",
    "    return \"|\".join(vids.tolist())\n",
    "\n",
    "g = df.groupby(author_col, dropna=False)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"author_id\": g.apply(lambda x: x.name),\n",
    "    \"video_count\": g.size().astype(int),\n",
    "    \"videos\": g.apply(join_videos),\n",
    "    \"avg_video_score\": g[\"_video_score\"].mean().round(6),\n",
    "    \"avg_spam_ratio\": g[\"_spam_ratio\"].mean().round(6),\n",
    "    \"avg_like_view_ratio\": g[\"_like_ratio\"].mean().round(6),\n",
    "    \"avg_comment_view_ratio\": g[\"_comment_ratio\"].mean().round(6),\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "out[\"main_category\"] = g.apply(resolve_main_category).values\n",
    "\n",
    "out = out.sort_values([\"video_count\", \"avg_video_score\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "out.to_csv(OUTPUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(out):,} creators → {OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends Analysis - Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT  = \"videos_scored.csv\"\n",
    "OUT_VIDS = \"Video_trends/top_combined_titles_by_category_filtered_nodup.csv\"\n",
    "OUT_TAGS = \"Video_trends/top_tags_by_category_filtered.csv\"\n",
    "TOP_K  = 20\n",
    "\n",
    "# thresholds\n",
    "LIKE_MIN    = 0.05\n",
    "COMMENT_MIN = 0.0005\n",
    "\n",
    "CATS = [\"makeup\",\"skincare\",\"fragrance\",\"hair\",\"skills\",\"nails\",\"fashion\",\"general lifestyle\"]\n",
    "\n",
    "def pick_col(df, names):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def to_int(s, default=0):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").fillna(default).astype(np.int64)\n",
    "\n",
    "def to_float(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "\n",
    "def normalize_category(s: str) > str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower().replace(\"_\",\" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def norm_title(s: str) > str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def parse_tags(cell: str):\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return []\n",
    "    txt = cell.strip().strip(\"[]\")\n",
    "    parts = re.split(r\"[|,]\", txt)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        t = p.strip().strip(\"'\").strip('\"')\n",
    "        t = re.sub(r\"\\s+\", \" \", t).lower()\n",
    "        if t:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "\n",
    "vid_col    = pick_col(df, [\"videoId\", \"video_id\"])\n",
    "title_col  = pick_col(df, [\"title\"])\n",
    "cat_col    = pick_col(df, [\"defining_category\", \"defining category\"])\n",
    "views_col  = pick_col(df, [\"viewCount\", \"views\"])\n",
    "likes_col  = pick_col(df, [\"likeCount\", \"likes\"])\n",
    "tags_col   = pick_col(df, [\"video_tags\", \"tags\"])\n",
    "\n",
    "if \"like_ratio\" in df.columns:\n",
    "    like_ratio = to_float(df[\"like_ratio\"]).fillna(0.0)\n",
    "else:\n",
    "    views = to_int(df[views_col], 0)\n",
    "    likes = to_int(df[likes_col], 0)\n",
    "    like_ratio = (likes / np.maximum(views, 1)).astype(float)\n",
    "\n",
    "if \"comment_ratio\" in df.columns:\n",
    "    comment_ratio = to_float(df[\"comment_ratio\"]).fillna(0.0)\n",
    "else:\n",
    "    v   = to_int(df[views_col], 0)\n",
    "    tot = to_int(df.get(\"total_comment_count\", 0))\n",
    "    nrm = to_int(df.get(\"normal_comment_count\", 0))\n",
    "    spm = to_int(df.get(\"spam_comment_count\", 0))\n",
    "    rep = to_int(df.get(\"commentCount\", 0))\n",
    "    used = np.maximum.reduce([tot.values, (nrm+spm).values, rep.values])\n",
    "    comment_ratio = (used / np.maximum(v.values, 1)).astype(float)\n",
    "\n",
    "df[\"_cat_norm\"]      = df[cat_col].map(normalize_category)\n",
    "df[\"_like_ratio\"]    = like_ratio\n",
    "df[\"_comment_ratio\"] = comment_ratio\n",
    "\n",
    "mask = (df[\"_like_ratio\"] >= LIKE_MIN) & (df[\"_comment_ratio\"] >= COMMENT_MIN)\n",
    "df_f = df.loc[mask].copy()\n",
    "df_f[\"_combined_ratio\"] = df_f[\"_like_ratio\"] + df_f[\"_comment_ratio\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for cat in CATS:\n",
    "    sub = df_f[df_f[\"_cat_norm\"] == normalize_category(cat)].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    sub = sub.sort_values(\"_combined_ratio\", ascending=False)\n",
    "\n",
    "    kept_rows = []\n",
    "    title_dupe_counts = {}\n",
    "    seen_titles = set()\n",
    "\n",
    "    for _, r in sub.iterrows():\n",
    "        t_norm = norm_title(r.get(title_col, \"\"))\n",
    "        if not t_norm:\n",
    "            continue\n",
    "\n",
    "        if t_norm in seen_titles:\n",
    "            title_dupe_counts[t_norm] = title_dupe_counts.get(t_norm, 0) + 1\n",
    "            continue\n",
    "\n",
    "        seen_titles.add(t_norm)\n",
    "        title_dupe_counts.setdefault(t_norm, 0)\n",
    "\n",
    "        kept_rows.append({\n",
    "            \"category\": cat,\n",
    "            \"videoId\": r.get(vid_col, \"\"),\n",
    "            \"title\": r.get(title_col, \"\"),\n",
    "            \"like_ratio\": float(r[\"_like_ratio\"]) if pd.notna(r[\"_like_ratio\"]) else np.nan,\n",
    "            \"comment_ratio\": float(r[\"_comment_ratio\"]) if pd.notna(r[\"_comment_ratio\"]) else np.nan,\n",
    "            \"combined_ratio\": float(r[\"_combined_ratio\"]) if pd.notna(r[\"_combined_ratio\"]) else np.nan,\n",
    "            \"dulicate_videos\": 0, \n",
    "        })\n",
    "\n",
    "        if len(kept_rows) >= TOP_K:\n",
    "            break\n",
    "\n",
    "    for row in kept_rows:\n",
    "        row[\"dulicate_videos\"] = title_dupe_counts.get(norm_title(row[\"title\"]), 0)\n",
    "\n",
    "    rows.extend(kept_rows)\n",
    "\n",
    "out_vids = pd.DataFrame(rows)\n",
    "out_vids.to_csv(OUT_VIDS, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(out_vids):,} rows → {OUT_VIDS}\")\n",
    "\n",
    "if tags_col is None:\n",
    "    print(\"No tags column found; skipping tag leaderboard.\")\n",
    "else:\n",
    "    df_tags = df_f[[tags_col, \"_cat_norm\"]].copy()\n",
    "    df_tags[\"__tags_list__\"] = df_tags[tags_col].apply(parse_tags)\n",
    "\n",
    "    df_ex = df_tags.explode(\"__tags_list__\")\n",
    "    df_ex = df_ex[df_ex[\"__tags_list__\"].notna() & (df_ex[\"__tags_list__\"].str.strip() != \"\")]\n",
    "    if df_ex.empty:\n",
    "        print(\"No usable tags after parsing; skipping tag leaderboard.\")\n",
    "    else:\n",
    "        tag_counts = (\n",
    "            df_ex.groupby([\"_cat_norm\", \"__tags_list__\"])\n",
    "                 .size()\n",
    "                 .reset_index(name=\"count\")\n",
    "                 .sort_values([\"_cat_norm\", \"count\"], ascending=[True, False])\n",
    "        )\n",
    "\n",
    "        top_tag_rows = []\n",
    "        for cat in CATS:\n",
    "            cat_norm = normalize_category(cat)\n",
    "            sub = tag_counts[tag_counts[\"_cat_norm\"] == cat_norm].head(20)\n",
    "            for _, r in sub.iterrows():\n",
    "                top_tag_rows.append({\n",
    "                    \"category\": cat,\n",
    "                    \"tag\": r[\"__tags_list__\"],\n",
    "                    \"count\": int(r[\"count\"]),\n",
    "                })\n",
    "\n",
    "        out_tags = pd.DataFrame(top_tag_rows)\n",
    "        out_tags.to_csv(OUT_TAGS, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Wrote {len(out_tags):,} tag rows → {OUT_TAGS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT  = \"unique_video_authors.csv\"\n",
    "OUTPUT = \"Video_trends/top_influencers_by_category.csv\"\n",
    "\n",
    "LIKE_MIN    = 0.05 \n",
    "COMMENT_MIN = 0.0005  \n",
    "\n",
    "def _norm_cat(s: str) > str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower().replace(\"_\",\" \")\n",
    "    return re.sub(r\"\\s+\",\" \", s)\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "\n",
    "need = [\n",
    "    \"author_id\",\"video_count\",\"videos\",\"avg_video_score\",\n",
    "    \"avg_spam_ratio\",\"avg_like_view_ratio\",\"avg_comment_view_ratio\",\"main_category\"\n",
    "]\n",
    "missing = [c for c in need if c not in df.columns]\n",
    "print(missing)\n",
    "\n",
    "df[\"video_count\"]              = pd.to_numeric(df[\"video_count\"], errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "df[\"avg_video_score\"]          = pd.to_numeric(df[\"avg_video_score\"], errors=\"coerce\").astype(float)\n",
    "df[\"avg_spam_ratio\"]           = pd.to_numeric(df[\"avg_spam_ratio\"], errors=\"coerce\").astype(float)\n",
    "df[\"avg_like_view_ratio\"]      = pd.to_numeric(df[\"avg_like_view_ratio\"], errors=\"coerce\").astype(float)\n",
    "df[\"avg_comment_view_ratio\"]   = pd.to_numeric(df[\"avg_comment_view_ratio\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "df[\"_cat_norm\"]   = df[\"main_category\"].map(_norm_cat)\n",
    "df[\"category\"]    = df[\"main_category\"]  # keep original label spelling/case as-is for output\n",
    "\n",
    "mask = (df[\"avg_like_view_ratio\"] >= LIKE_MIN) & (df[\"avg_comment_view_ratio\"] >= COMMENT_MIN)\n",
    "df_pass = df.loc[mask].copy()\n",
    "\n",
    "tops = []\n",
    "for cat_norm, sub in df_pass.groupby(\"_cat_norm\", dropna=False):\n",
    "    sub = sub.sort_values(\n",
    "        by=[\"avg_video_score\",\"video_count\"],\n",
    "        ascending=[False, False]\n",
    "    ).head(20).copy()\n",
    "    sub.insert(0, \"rank_in_category\", range(1, len(sub) + 1))\n",
    "    tops.append(sub)\n",
    "\n",
    "if tops:\n",
    "    out = pd.concat(tops, ignore_index=True)\n",
    "\n",
    "    for c in [\"avg_like_view_ratio\",\"avg_comment_view_ratio\",\"avg_video_score\",\"avg_spam_ratio\"]:\n",
    "        out[c] = out[c].round(6)\n",
    "\n",
    "    cols = [\n",
    "        \"rank_in_category\",\"category\",\"author_id\",\"video_count\",\n",
    "        \"avg_like_view_ratio\",\"avg_comment_view_ratio\",\"avg_spam_ratio\",\"avg_video_score\",\n",
    "        \"videos\"\n",
    "    ]\n",
    "    out = out[cols]\n",
    "\n",
    "    out.to_csv(OUTPUT, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote {len(out):,} rows → {OUTPUT}\")\n",
    "else:\n",
    "    print(\"No authors met the average ratio thresholds; no top list written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT  = \"videos_scored.csv\"\n",
    "OUTPUT = \"Video_trends/top_video_features_by_category.csv\"\n",
    "\n",
    "CATS = [\"makeup\",\"skincare\",\"fragrance\",\"hair\",\"skills\",\"nails\",\"fashion\",\"general lifestyle\"]\n",
    "\n",
    "LIKE_MIN    = 0.05\n",
    "COMMENT_MIN = 0.0005\n",
    "SHORTFORM_CUTOFF_SEC = 3 * 60\n",
    "\n",
    "CAT_COL   = \"defining_category\"\n",
    "VIEWS_COL = \"viewCount\"\n",
    "LIKES_COL = \"likeCount\"\n",
    "CMT_META  = \"commentCount\"\n",
    "CMT_NORM  = \"normal_comment_count\"\n",
    "CMT_SPAM  = \"spam_comment_count\"\n",
    "CMT_TOTAL = \"total_comment_count\"\n",
    "DUR_COL   = \"contentDuration\"\n",
    "\n",
    "def _to_int(s, default=0):\n",
    "    return pd.to_numeric(s, errors=\"coerce\").fillna(default).astype(np.int64)\n",
    "\n",
    "def _norm_cat(s: str) > str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower().replace(\"_\",\" \")\n",
    "    return re.sub(r\"\\s+\",\" \", s)\n",
    "\n",
    "_iso = re.compile(\n",
    "    r\"^P(?:(?P<days>\\d+)D)?(?:T(?:(?P<hours>\\d+)H)?(?:(?P<minutes>\\d+)M)?(?:(?P<seconds>\\d+)S)?)?$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def iso8601_to_seconds(s):\n",
    "    if not isinstance(s, str):\n",
    "        return np.nan\n",
    "    s = s.strip()\n",
    "    m = _iso.match(s)\n",
    "    if not m:\n",
    "        return np.nan\n",
    "    d = int(m.group(\"days\") or 0)\n",
    "    h = int(m.group(\"hours\") or 0)\n",
    "    mnt = int(m.group(\"minutes\") or 0)\n",
    "    sec = int(m.group(\"seconds\") or 0)\n",
    "    return d*86400 + h*3600 + mnt*60 + sec\n",
    "\n",
    "def series_stats(x: pd.Series):\n",
    "    if x.empty:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    return (x.min(), x.median(), x.mean(), x.max())\n",
    "\n",
    "def fmt_mm_ss(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    sec = int(round(float(val)))\n",
    "    if sec < 60:\n",
    "        return f\"{sec}S\"\n",
    "    m, s = divmod(sec, 60)\n",
    "    return f\"{m}M {s}S\"\n",
    "\n",
    "df = pd.read_csv(INPUT, engine=\"python\", on_bad_lines=\"skip\", dtype=str)\n",
    "\n",
    "canon = {_norm_cat(c): c for c in CATS}\n",
    "df[\"_cat_norm\"] = df[CAT_COL].map(_norm_cat)\n",
    "df = df[df[\"_cat_norm\"].isin(canon.keys())].copy()\n",
    "df[CAT_COL] = df[\"_cat_norm\"].map(canon)\n",
    "\n",
    "views = _to_int(df.get(VIEWS_COL, 0))\n",
    "likes = _to_int(df.get(LIKES_COL, 0))\n",
    "c_rep = _to_int(df.get(CMT_META, 0))\n",
    "n_norm = _to_int(df.get(CMT_NORM, 0))\n",
    "n_spam = _to_int(df.get(CMT_SPAM, 0))\n",
    "n_total = _to_int(df.get(CMT_TOTAL, 0)) if (CMT_TOTAL in df.columns) else (n_norm + n_spam)\n",
    "\n",
    "comments_used = pd.Series(\n",
    "    np.maximum.reduce([c_rep.values, n_total.values, (n_norm + n_spam).values]),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "like_ratio    = likes / np.maximum(views, 1)\n",
    "comment_ratio = comments_used / np.maximum(views, 1)\n",
    "\n",
    "dur_sec = df.get(DUR_COL, pd.Series(index=df.index, dtype=object)).apply(iso8601_to_seconds)\n",
    "\n",
    "df[\"like_ratio\"] = like_ratio\n",
    "df[\"comment_ratio\"] = comment_ratio\n",
    "df[\"_dur_sec\"] = dur_sec\n",
    "\n",
    "rows = []\n",
    "for cat in CATS:\n",
    "    sub = df[df[CAT_COL] == cat].copy()\n",
    "    if sub.empty:\n",
    "        rows.append({\n",
    "            \"category\": cat,\n",
    "            \"no_of_top_vids\": 0,\n",
    "            \"short_count\": 0,\n",
    "            \"long_count\": 0,\n",
    "            \"short_to_long_ratio\": np.nan,\n",
    "            \"short_min\": \"\", \"short_median\": \"\", \"short_mean\": \"\", \"short_max\": \"\",\n",
    "            \"long_min\": \"\", \"long_median\": \"\", \"long_mean\": \"\", \"long_max\": \"\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    ok = (sub[\"like_ratio\"] >= LIKE_MIN) & (sub[\"comment_ratio\"] >= COMMENT_MIN)\n",
    "    sub_ok = sub.loc[ok].copy()\n",
    "\n",
    "    if sub_ok.empty:\n",
    "        rows.append({\n",
    "            \"category\": cat,\n",
    "            \"no_of_top_vids\": 0,\n",
    "            \"short_count\": 0,\n",
    "            \"long_count\": 0,\n",
    "            \"short_to_long_ratio\": np.nan,\n",
    "            \"short_min\": \"\", \"short_median\": \"\", \"short_mean\": \"\", \"short_max\": \"\",\n",
    "            \"long_min\": \"\", \"long_median\": \"\", \"long_mean\": \"\", \"long_max\": \"\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    sub_ok[\"_eng_key\"] = sub_ok[\"like_ratio\"].astype(float) + sub_ok[\"comment_ratio\"].astype(float)\n",
    "    sub_ok[\"_views\"] = views.loc[sub_ok.index].astype(int)\n",
    "    sub_top = sub_ok.sort_values(by=[\"_eng_key\",\"_views\"], ascending=[False, False]).head(300).copy()\n",
    "\n",
    "    n_top = len(sub_top)\n",
    "\n",
    "    short_mask = (sub_top[\"_dur_sec\"] <= SHORTFORM_CUTOFF_SEC)\n",
    "    long_mask  = (sub_top[\"_dur_sec\"] >  SHORTFORM_CUTOFF_SEC)\n",
    "\n",
    "    short_secs = sub_top.loc[short_mask, \"_dur_sec\"].dropna().astype(float)\n",
    "    long_secs  = sub_top.loc[long_mask,  \"_dur_sec\"].dropna().astype(float)\n",
    "\n",
    "    short_count = int(short_mask.sum())\n",
    "    long_count  = int(long_mask.sum())\n",
    "    ratio = (short_count / long_count) if long_count > 0 else (np.inf if short_count > 0 else np.nan)\n",
    "\n",
    "    s_min, s_med, s_mean, s_max = series_stats(short_secs)\n",
    "    l_min, l_med, l_mean, l_max = series_stats(long_secs)\n",
    "\n",
    "    rows.append({\n",
    "        \"category\": cat,\n",
    "        \"no_of_top_vids\": n_top,\n",
    "        \"short_count\": short_count,\n",
    "        \"long_count\": long_count,\n",
    "        \"short_to_long_ratio\": round(ratio, 6) if np.isfinite(ratio) else (\"inf\" if short_count>0 and long_count==0 else np.nan),\n",
    "    \n",
    "        \"short_min\":   fmt_mm_ss(s_min),\n",
    "        \"short_median\":fmt_mm_ss(s_med),\n",
    "        \"short_mean\":  fmt_mm_ss(s_mean),\n",
    "        \"short_max\":   fmt_mm_ss(s_max),\n",
    "        \"long_min\":    fmt_mm_ss(l_min),\n",
    "        \"long_median\": fmt_mm_ss(l_med),\n",
    "        \"long_mean\":   fmt_mm_ss(l_mean),\n",
    "        \"long_max\":    fmt_mm_ss(l_max),\n",
    "    })\n",
    "\n",
    "out = pd.DataFrame(rows, columns=[\n",
    "    \"category\",\"no_of_top_vids\",\n",
    "    \"short_count\",\"long_count\",\"short_to_long_ratio\",\n",
    "    \"short_min\",\"short_median\",\"short_mean\",\"short_max\",\n",
    "    \"long_min\",\"long_median\",\"long_mean\",\"long_max\",\n",
    "])\n",
    "out.to_csv(OUTPUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(out)} rows → {OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommentSense Dashboard Outputs\n",
    "We could not work on displaying these too well due to time constraints, but it's likely the code from the Trends Analysis section would be used behind a website with better graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose category\n",
    "CATEGORY = \"makeup\"  # \"makeup\",\"skincare\",\"fragrance\",\"hair\",\"skills\",\"nails\",\"fashion\",\"general lifestyle\"\n",
    "\n",
    "PATH_TAGS      = \"Video_trends/top_tags_by_category_filtered.csv\"\n",
    "PATH_TITLES    = \"Video_trends/top_combined_titles_by_category_filtered_nodup.csv\"\n",
    "PATH_INFLU     = \"Video_trends/top_influencers_by_category.csv\"\n",
    "PATH_FEATURES  = \"Video_trends/top_video_features_by_category.csv\"\n",
    "\n",
    "def _lower_norm(s): \n",
    "    return str(s).strip().lower()\n",
    "\n",
    "def _pick_col(df, options, required=True):\n",
    "    for c in options:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"None of {options} found. Available: {list(df.columns)}\")\n",
    "    return None\n",
    "\n",
    "def _as_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _wrap(labels, width=28):\n",
    "    return [textwrap.fill(str(x), width=width) for x in labels]\n",
    "\n",
    "PASTELS = [\n",
    "    \"#A6CEE3\",\"#B2DF8A\",\"#FDBF6F\",\"#CAB2D6\",\"#FB9A99\",\"#F4C2C2\",\n",
    "    \"#CCEBC5\",\"#B3CDE3\",\"#DECBE4\",\"#FED9A6\",\"#FFFFCC\",\"#E5D8BD\"\n",
    "]\n",
    "\n",
    "df_tags   = pd.read_csv(PATH_TAGS)\n",
    "df_titles = pd.read_csv(PATH_TITLES)\n",
    "df_influ  = pd.read_csv(PATH_INFLU)\n",
    "df_feat   = pd.read_csv(PATH_FEATURES)\n",
    "\n",
    "for df in (df_tags, df_titles, df_influ, df_feat):\n",
    "    if \"category\" not in df.columns:\n",
    "        for alt in [\"defining_category\", \"defining category\", \"cat\", \"Category\"]:\n",
    "            if alt in df.columns:\n",
    "                df.rename(columns={alt: \"category\"}, inplace=True)\n",
    "                break\n",
    "    df[\"category_norm\"] = df[\"category\"].map(_lower_norm)\n",
    "\n",
    "cat_key = _lower_norm(CATEGORY)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    specs=[[{\"type\":\"domain\"}],# row 1: pie\n",
    "           [{\"type\":\"xy\"}],# row 2: bar\n",
    "           [{\"type\":\"domain\"}],# row 3: pie\n",
    "           [{\"type\":\"xy\"}]],# row 4: text panel via blank scatter\n",
    "    vertical_spacing=0.08,\n",
    "    subplot_titles=(\n",
    "        f\"Top 10 Tags — {CATEGORY}\",\n",
    "        f\"Top 10 Titles — {CATEGORY}\",\n",
    "        f\"Short vs Long Videos — {CATEGORY}\",# medians appended later\n",
    "        f\"Top 10 Influencers — {CATEGORY}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1) Top 10 tags (pie)\n",
    "try:\n",
    "    tag_col   = _pick_col(df_tags, [\"tag\",\"tags\",\"keyword\"])\n",
    "    count_col = _pick_col(df_tags, [\"count\",\"tag_count\",\"frequency\",\"freq\"])\n",
    "\n",
    "    tags_cat = df_tags.loc[df_tags[\"category_norm\"] == cat_key, [tag_col, count_col]].copy()\n",
    "    tags_cat[count_col] = _as_num(tags_cat[count_col]).fillna(0)\n",
    "    tags_top10 = tags_cat.sort_values(count_col, ascending=False).head(10)\n",
    "\n",
    "    if tags_top10.empty:\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=[\"(no data)\"], values=[1],\n",
    "                marker=dict(colors=[PASTELS[0]]),\n",
    "                showlegend=False, textinfo=\"label+percent\", textposition=\"outside\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    else:\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=_wrap(tags_top10[tag_col].values, width=18),\n",
    "                values=tags_top10[count_col].values,\n",
    "                marker=dict(colors=PASTELS),\n",
    "                showlegend=False,                # <- prevent labels at bottom\n",
    "                textinfo=\"label+percent\",        # <- show label + %\n",
    "                textposition=\"outside\",          # <- labels beside wedges\n",
    "                hovertemplate=\"%{label}<br>%{value} hits<extra></extra>\",\n",
    "                sort=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "except Exception as e:\n",
    "    fig.add_trace(go.Pie(labels=[f\"Error: {e}\"], values=[1], marker=dict(colors=[PASTELS[5]]),\n",
    "                         showlegend=False, textinfo=\"label+percent\", textposition=\"outside\"),\n",
    "                  row=1, col=1)\n",
    "    \n",
    "\n",
    "# 2) Top 10 titles \n",
    "\n",
    "try:\n",
    "    title_col = _pick_col(df_titles, [\"title\",\"video_title\"])\n",
    "    like_ratio_col = _pick_col(\n",
    "        df_titles, [\"like_ratio\",\"like_view_ratio\",\"avg_like_view_ratio\",\"lv\",\"like_to_view_ratio\"]\n",
    "    )\n",
    "    comment_ratio_col = _pick_col(\n",
    "        df_titles, [\"comment_ratio\",\"comment_view_ratio\",\"avg_comment_view_ratio\",\"cv\",\"comment_to_view_ratio\"]\n",
    "    )\n",
    "\n",
    "    sum_col = \"sum_like_comment_ratio\"\n",
    "    if sum_col not in df_titles.columns:\n",
    "        df_titles[sum_col] = _as_num(df_titles[like_ratio_col]) + _as_num(df_titles[comment_ratio_col])\n",
    "\n",
    "    titles_cat = df_titles.loc[df_titles[\"category_norm\"] == cat_key, [title_col, sum_col]].copy()\n",
    "    titles_cat[sum_col] = _as_num(titles_cat[sum_col]).fillna(0)\n",
    "    titles_top10 = titles_cat.sort_values(sum_col, ascending=False).head(10)\n",
    "\n",
    "    if titles_top10.empty:\n",
    "        fig.add_trace(go.Bar(x=[0], y=[\"(no data)\"], orientation=\"h\", marker=dict(color=PASTELS[1])), row=2, col=1)\n",
    "    else:\n",
    "        # reverse for horizontal bar (largest at top)\n",
    "        y_labels = _wrap(titles_top10[title_col].values, width=60)[::-1]\n",
    "        x_vals   = titles_top10[sum_col].values[::-1]\n",
    "        colors   = (PASTELS * ((len(y_labels)//len(PASTELS))+1))[:len(y_labels)]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=x_vals,\n",
    "                y=y_labels,\n",
    "                orientation=\"h\",\n",
    "                marker=dict(color=colors, line=dict(color=\"rgba(0,0,0,0)\", width=0.5)),\n",
    "                hovertemplate=\"<b>%{y}</b><br>sum(like/view + comment/view)=%{x:.4f}<extra></extra>\"\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig.update_yaxes(automargin=True, row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"like/view + comment/view (sum)\", row=2, col=1)\n",
    "except Exception as e:\n",
    "    fig.add_trace(go.Bar(x=[0], y=[f\"Error: {e}\"], orientation=\"h\", marker=dict(color=PASTELS[5])), row=2, col=1)\n",
    "\n",
    "\n",
    "# 3) Short vs Long (pie) \n",
    "median_note = None\n",
    "try:\n",
    "    short_ct_col = _pick_col(df_feat, [\"short_count\",\"shortform_count\",\"num_short\"])\n",
    "    long_ct_col  = _pick_col(df_feat, [\"long_count\",\"longform_count\",\"num_long\"])\n",
    "    short_med    = _pick_col(df_feat, [\"short_median\",\"shortform_median\"], required=False)\n",
    "    long_med     = _pick_col(df_feat, [\"long_median\",\"longform_median\"], required=False)\n",
    "\n",
    "    feat_row = df_feat.loc[df_feat[\"category_norm\"] == cat_key]\n",
    "    if feat_row.empty:\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=[\"(no data)\"], values=[1], marker=dict(colors=[PASTELS[2]]),\n",
    "                   showlegend=False, textinfo=\"label+percent\", textposition=\"outside\"),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    else:\n",
    "        r = feat_row.iloc[0]\n",
    "        def _to_int(v):\n",
    "            try: return int(float(v))\n",
    "            except Exception: return 0\n",
    "        short_n = _to_int(r[short_ct_col])\n",
    "        long_n  = _to_int(r[long_ct_col])\n",
    "        smed = (r[short_med] if short_med and pd.notna(r[short_med]) else None)\n",
    "        lmed = (r[long_med]  if long_med  and pd.notna(r[long_med])  else None)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=[\"Shortform (≤3 min)\", \"Longform (>3 min)\"],\n",
    "                values=[short_n, long_n],\n",
    "                marker=dict(colors=[PASTELS[0], PASTELS[3]]),\n",
    "                showlegend=False,               # <- no legend below\n",
    "                textinfo=\"label+percent\",       # <- show label + %\n",
    "                textposition=\"outside\",         # <- labels beside wedges\n",
    "                hovertemplate=\"%{label}<br>%{value} videos<extra></extra>\",\n",
    "                sort=False\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "\n",
    "        parts = []\n",
    "        if smed: parts.append(f\"Short median: {smed}\")\n",
    "        if lmed: parts.append(f\"Long median: {lmed}\")\n",
    "        if parts:\n",
    "            median_note = \" | \".join(parts)\n",
    "except Exception as e:\n",
    "    fig.add_trace(go.Pie(labels=[f\"Error: {e}\"], values=[1], marker=dict(colors=[PASTELS[5]]),\n",
    "                         showlegend=False, textinfo=\"label+percent\", textposition=\"outside\"),\n",
    "                  row=3, col=1)\n",
    "    \n",
    "\n",
    "# 4) Top 10 influencers\n",
    "try:\n",
    "    auth_col  = _pick_col(df_influ, [\"author_id\",\"creator_id\",\"channel_id\"])\n",
    "    score_col = _pick_col(df_influ, [\"avg_video_score\",\"avg_score\",\"video_score_mean\"])\n",
    "    alv_col   = _pick_col(df_influ, [\"avg_like_view_ratio\",\"avg_like_to_view\",\"lv_avg\"])\n",
    "    acv_col   = _pick_col(df_influ, [\"avg_comment_view_ratio\",\"avg_comment_to_view\",\"cv_avg\"])\n",
    "\n",
    "    infl_cat = df_influ.loc[df_influ[\"category_norm\"] == cat_key, [auth_col, score_col, alv_col, acv_col]].copy()\n",
    "    infl_cat[alv_col] = _as_num(infl_cat[alv_col])\n",
    "    infl_cat[acv_col] = _as_num(infl_cat[acv_col])\n",
    "    infl_cat[score_col] = _as_num(infl_cat[score_col])\n",
    "\n",
    "    infl_cat = infl_cat[(infl_cat[alv_col] >= 0.05) & (infl_cat[acv_col] >= 0.0005)]\n",
    "    infl_top10 = infl_cat.sort_values(score_col, ascending=False).head(10)\n",
    "\n",
    "    if infl_top10.empty:\n",
    "        list_text = \"(none after engagement filters)\"\n",
    "    else:\n",
    "        lines = []\n",
    "        for i, row in enumerate(infl_top10.itertuples(index=False), start=1):\n",
    "            author = getattr(row, auth_col)\n",
    "            avg_sc = getattr(row, score_col)\n",
    "            alv    = getattr(row, alv_col)\n",
    "            acv    = getattr(row, acv_col)\n",
    "            lines.append(f\"{i:>2}. {author} | score={avg_sc:.3f} | like/view={alv:.2%} | comment/view={acv:.3%}\")\n",
    "        list_text = \"<br>\".join(lines)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[0], y=[0], mode=\"markers\", marker=dict(opacity=0)), row=4, col=1)\n",
    "    fig.update_xaxes(visible=False, row=4, col=1)\n",
    "    fig.update_yaxes(visible=False, row=4, col=1)\n",
    "    fig.add_annotation(\n",
    "        row=4, col=1, x=0.5, y=0.5, xref=\"x domain\", yref=\"y domain\",\n",
    "        text=list_text, showarrow=False, align=\"left\",\n",
    "        font=dict(family=\"Courier New, monospace\", size=12)\n",
    "    )\n",
    "except Exception as e:\n",
    "    fig.add_trace(go.Scatter(x=[0], y=[0], mode=\"markers\", marker=dict(opacity=0)), row=4, col=1)\n",
    "    fig.add_annotation(\n",
    "        row=4, col=1, x=0.5, y=0.5, xref=\"x domain\", yref=\"y domain\",\n",
    "        text=f\"Influencers error: {e}\", showarrow=False\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1100, height=2100,\n",
    "    title=dict(text=f\"Category Insights — {CATEGORY}\", x=0.5, xanchor=\"center\"),\n",
    "    margin=dict(l=60, r=60, t=80, b=60),\n",
    "    uniformtext_minsize=10, \n",
    "    uniformtext_mode=\"hide\" \n",
    ")\n",
    "\n",
    "for ann in fig['layout']['annotations']:\n",
    "    if isinstance(ann.text, str) and ann.text.startswith((\"Top 10 Tags\", \"Top 10 Titles\", \"Short vs Long\", \"Top 10 Influencers\")):\n",
    "        ann.x = 0.5\n",
    "        ann.xanchor = 'center'\n",
    "\n",
    "if median_note:\n",
    "    for ann in fig['layout']['annotations']:\n",
    "        if isinstance(ann.text, str) and ann.text.startswith(\"Short vs Long\"):\n",
    "            ann.text = ann.text + f\"<br><sup>{median_note}</sup>\"\n",
    "            break\n",
    "\n",
    "fig.update_traces(selector=dict(type=\"pie\"), showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_int_maybe(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"[^\\d]\", \"\", s) \n",
    "    return int(s) if s else None\n",
    "\n",
    "def classify_creator_size(subscribers):\n",
    "    n = _to_int_maybe(subscribers)\n",
    "    if n is None:\n",
    "        return \"Unknown\"\n",
    "    if n >= 1_000_000:\n",
    "        return \"Mega Influencer\"\n",
    "    if n >= 100_000:\n",
    "        return \"Macro Influencer\"\n",
    "    if n >= 1_000:\n",
    "        return \"Micro Influencer\"\n",
    "    return \"Nano Influencer\"\n",
    "\n",
    "\n",
    "#example IDS: 6084, 12942\n",
    "\n",
    "CREATOR_ID = \"12942\"\n",
    "\n",
    "CREATOR_META = {\n",
    "    \"12942\": {\n",
    "        #these details are not in the sheet, so they were manually taken. In future, they can be scraped alongside the other data\n",
    "        \"name\": \"LiVing Ash\", #\"Glam up with Tanya\",\n",
    "        \"subscribers\": 130000, #2480,\n",
    "        \"classification\": \"Macro\",\n",
    "        \"total_videos\": 417,\n",
    "        \"profile_img\": \"https://yt3.googleusercontent.com/9pyt-iT19kqGtimYSDvp_GFWkq69BrCPD3EFBrufTVzoYUPx3_3jxN7ILhzCcpvsEvkSo4Rj=s160-c-k-c0x00ffffff-no-rj\",\n",
    "        \"banner_img\": \"https://yt3.googleusercontent.com/oBMdgbh1tw-OHOq-xpa5sWEqm4fBhahAO_JDFVmo4B7mDw_0s1ql8pFVlRlrawh8S67dmeTgxg=w1138-fcrop64=1,00005a57ffffa5a8-k-c0xffffffff-no-nd-rj\",\n",
    "        \"channel_url\": \"https://www.youtube.com/channel/UCEGqJWjAIZvUoZ-SimGdkpA\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# CSV paths\n",
    "PATH_UNIQUE_AUTHORS = \"unique_video_authors.csv\"\n",
    "PATH_VIDEOS_SCORED  = \"videos_scored.csv\"\n",
    "\n",
    "# How many videos to list (None = all)\n",
    "MAX_VIDEOS_LIST = 25\n",
    "\n",
    "# Pastel palette\n",
    "PASTELS = [\"#a3c4f3\",\"#c2eabd\",\"#ffd6a5\",\"#ffadad\",\"#bdb2ff\",\"#bde0fe\",\"#ffc8dd\",\"#caffbf\",\"#ffe5a5\"]\n",
    "\n",
    "def _as_num(s): \n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _pct(x, places=2):\n",
    "    try: return f\"{float(x):.{places}%}\"\n",
    "    except: return \"\"\n",
    "\n",
    "def _wrap(vals, width=38):\n",
    "    return [textwrap.fill(str(v), width=width) for v in vals]\n",
    "\n",
    "def _br_wrap(s, every=50):\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    return \"<br>\".join(textwrap.wrap(s, every)) if s else \"\"\n",
    "\n",
    "def parse_videos_list(s):\n",
    "    if not isinstance(s, str): return []\n",
    "    return [v.strip() for v in s.split(\"|\") if v.strip()]\n",
    "\n",
    "def ensure_ratios(df):\n",
    "    if \"like_ratio\" not in df.columns:\n",
    "        if (\"likeCount\" in df.columns) and (\"viewCount\" in df.columns):\n",
    "            likes = _as_num(df[\"likeCount\"]).fillna(0)\n",
    "            views = _as_num(df[\"viewCount\"]).fillna(0).replace(0, np.nan)\n",
    "            df[\"like_ratio\"] = (likes / views).fillna(0.0)\n",
    "        else:\n",
    "            df[\"like_ratio\"] = np.nan\n",
    "    if \"comment_ratio\" not in df.columns:\n",
    "        if \"commentCount\" in df.columns:\n",
    "            comments = _as_num(df[\"commentCount\"]).fillna(0)\n",
    "        elif \"total_comment_count\" in df.columns:\n",
    "            comments = _as_num(df[\"total_comment_count\"]).fillna(0)\n",
    "        else:\n",
    "            comments = pd.Series(0, index=df.index, dtype=float)\n",
    "        views = _as_num(df.get(\"viewCount\", 0)).fillna(0).replace(0, np.nan)\n",
    "        df[\"comment_ratio\"] = (comments / views).fillna(0.0)\n",
    "    if \"video_score\" not in df.columns:\n",
    "        for alt in [\"score\", \"final_score\", \"videoScore\", \"video_score_final\"]:\n",
    "            if alt in df.columns:\n",
    "                df.rename(columns={alt: \"video_score\"}, inplace=True); break\n",
    "    if \"video_score\" not in df.columns:\n",
    "        df[\"video_score\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def header_block_text(meta, row):\n",
    "    lines = []\n",
    "    if meta:\n",
    "        name = meta.get(\"name\", \"\")\n",
    "        subs = meta.get(\"subscribers\", \"\")\n",
    "        classification = meta.get(\"classification\", \"\")\n",
    "        total = meta.get(\"total_videos\", \"\")\n",
    "        url = meta.get(\"channel_url\", \"\")\n",
    "        if name: lines.append(f\"<b>{name}</b>\")\n",
    "        if subs != \"\":  lines.append(f\"Subscribers: {subs} (Size: {classification})\" if isinstance(subs, int) else f\"Subscribers: {subs}\")\n",
    "        if total != \"\": lines.append(f\"Total videos: {total}\")\n",
    "        if url:         lines.append(f'<a href=\"{url}\">{url}</a>')\n",
    "    if row is not None:\n",
    "        lines.append(f\"Videos in system: {row.get('video_count', 0)}\")\n",
    "        lines.append(f\"Main category: {row.get('main_category','')}\")\n",
    "        lines.append(f\"Avg video score: {row.get('avg_video_score','')}\")\n",
    "        lines.append(f\"Avg like/view: {_pct(row.get('avg_like_view_ratio', np.nan))}\")\n",
    "        lines.append(f\"Avg comment/view: {_pct(row.get('avg_comment_view_ratio', np.nan))}\")\n",
    "        lines.append(f\"Avg spam ratio: {_pct(row.get('avg_spam_ratio', np.nan))}\")\n",
    "    return \"<br>\".join(lines)\n",
    "\n",
    "ua = pd.read_csv(PATH_UNIQUE_AUTHORS, dtype=str)\n",
    "vs = pd.read_csv(PATH_VIDEOS_SCORED, dtype=str)\n",
    "\n",
    "# Coerce numerics in authors\n",
    "for col in [\"video_count\",\"avg_video_score\",\"avg_spam_ratio\",\"avg_like_view_ratio\",\"avg_comment_view_ratio\"]:\n",
    "    if col in ua.columns:\n",
    "        ua[col] = _as_num(ua[col])\n",
    "\n",
    "if \"author_id\" not in ua.columns:\n",
    "    for alt in [\"channel_id\",\"creator_id\",\"authorId\",\"channelId\"]:\n",
    "        if alt in ua.columns:\n",
    "            ua.rename(columns={alt: \"author_id\"}, inplace=True); break\n",
    "\n",
    "row_ua = ua.loc[ua[\"author_id\"] == CREATOR_ID]\n",
    "row_dict = None; video_ids = []\n",
    "if not row_ua.empty:\n",
    "    row_ua = row_ua.iloc[0]\n",
    "    row_dict = {\n",
    "        \"video_count\": int(_as_num(row_ua.get(\"video_count\", 0)) or 0),\n",
    "        \"main_category\": row_ua.get(\"main_category\", \"\"),\n",
    "        \"avg_video_score\": (lambda x: f\"{float(x):.3f}\" if pd.notna(x) else \"\")(row_ua.get(\"avg_video_score\", np.nan)),\n",
    "        \"avg_spam_ratio\": float(row_ua.get(\"avg_spam_ratio\", np.nan)) if pd.notna(row_ua.get(\"avg_spam_ratio\", np.nan)) else np.nan,\n",
    "        \"avg_like_view_ratio\": float(row_ua.get(\"avg_like_view_ratio\", np.nan)) if pd.notna(row_ua.get(\"avg_like_view_ratio\", np.nan)) else np.nan,\n",
    "        \"avg_comment_view_ratio\": float(row_ua.get(\"avg_comment_view_ratio\", np.nan)) if pd.notna(row_ua.get(\"avg_comment_view_ratio\", np.nan)) else np.nan,\n",
    "    }\n",
    "    if \"videos\" in ua.columns:\n",
    "        video_ids = parse_videos_list(row_ua[\"videos\"])\n",
    "\n",
    "vs = ensure_ratios(vs)\n",
    "if \"videoId\" not in vs.columns:\n",
    "    for alt in [\"video_id\",\"id\",\"VideoID\"]:\n",
    "        if alt in vs.columns:\n",
    "            vs.rename(columns={alt: \"videoId\"}, inplace=True); break\n",
    "\n",
    "vids_df = vs.loc[vs[\"videoId\"].isin(video_ids)].copy() if video_ids else vs.iloc[0:0].copy()\n",
    "\n",
    "for col in [\"video_score\",\"spam_ratio\",\"comment_score_mean\",\"like_ratio\",\"comment_ratio\",\"viewCount\"]:\n",
    "    if col in vids_df.columns:\n",
    "        vids_df[col] = _as_num(vids_df[col])\n",
    "\n",
    "title_col = \"title\" if \"title\" in vids_df.columns else (\"video_title\" if \"video_title\" in vids_df.columns else None)\n",
    "cat_col   = \"defining_category\"\n",
    "\n",
    "table_df = vids_df[[c for c in [\"videoId\", title_col, cat_col, \"video_score\", \"spam_ratio\", \"comment_score_mean\", \"like_ratio\", \"comment_ratio\"] if c and c in vids_df.columns]].copy()\n",
    "if title_col in table_df.columns:\n",
    "    table_df[title_col] = table_df[title_col].apply(lambda s: _br_wrap(s, 60))\n",
    "for pcol in [\"like_ratio\",\"comment_ratio\",\"spam_ratio\"]:\n",
    "    if pcol in table_df.columns:\n",
    "        table_df[pcol] = table_df[pcol].apply(lambda x: _pct(x, 2) if pd.notna(x) else \"\")\n",
    "if \"video_score\" in table_df.columns:\n",
    "    table_df[\"video_score\"] = table_df[\"video_score\"].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"\")\n",
    "if MAX_VIDEOS_LIST is not None and len(table_df) > MAX_VIDEOS_LIST:\n",
    "    table_df = table_df.head(MAX_VIDEOS_LIST)\n",
    "\n",
    "cat_counts = vids_df[cat_col].fillna(\"(unknown)\").value_counts() if cat_col else pd.Series([], dtype=int)\n",
    "\n",
    "scatter_df = vids_df.dropna(subset=[\"like_ratio\",\"comment_ratio\"])\n",
    "sizes = (scatter_df[\"video_score\"].fillna(0.5) * 6 + 3) if \"video_score\" in scatter_df.columns else pd.Series(7, index=scatter_df.index)\n",
    "sizes = np.clip(sizes, 4, 10)  # smaller dots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=6, cols=1,\n",
    "    specs=[\n",
    "        [{\"type\":\"xy\"}],       # 1: overview text\n",
    "        [{\"type\":\"xy\"}],       # 2: profile + banner\n",
    "        [{\"type\":\"domain\"}],   # 3: category pie\n",
    "        [{\"type\":\"xy\"}],       # 4: engagement scatter\n",
    "        [{\"type\":\"table\"}],    # 5: table part 1\n",
    "        [{\"type\":\"table\"}],    # 6: table part 2\n",
    "    ],\n",
    "    row_heights=[0.11, 0.16, 0.20, 0.19, 0.17, 0.17],\n",
    "    vertical_spacing=0.05,\n",
    ")\n",
    "\n",
    "meta = CREATOR_META.get(CREATOR_ID, {})\n",
    "header_text = header_block_text(meta, row_dict)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0.5], y=[0.5], mode=\"text\", text=[header_text],\n",
    "    textposition=\"middle center\", textfont=dict(size=15),\n",
    "    hoverinfo=\"skip\", showlegend=False\n",
    "), row=1, col=1)\n",
    "fig.update_xaxes(visible=False, row=1, col=1)\n",
    "fig.update_yaxes(visible=False, row=1, col=1)\n",
    "\n",
    "if cat_counts.empty:\n",
    "    fig.add_trace(go.Pie(labels=[\"(no videos)\"], values=[1],\n",
    "                         marker=dict(colors=[PASTELS[0]]),\n",
    "                         textinfo=\"label+percent\", textposition=\"outside\",\n",
    "                         showlegend=False, sort=False),\n",
    "                  row=3, col=1)\n",
    "else:\n",
    "    fig.add_trace(go.Pie(labels=cat_counts.index.tolist(),\n",
    "                         values=cat_counts.values.tolist(),\n",
    "                         marker=dict(colors=PASTELS),\n",
    "                         textinfo=\"label+percent\",\n",
    "                         textposition=\"outside\",\n",
    "                         showlegend=False, sort=False),\n",
    "                  row=3, col=1)\n",
    "\n",
    "#  Row 4: Engagement scatter (smaller dots, auto margins) ----\n",
    "if scatter_df.empty:\n",
    "    fig.add_trace(go.Scatter(x=[0], y=[0], mode=\"text\",\n",
    "                             text=[\"(no videos with ratios)\"],\n",
    "                             showlegend=False, hoverinfo=\"skip\"),\n",
    "                  row=4, col=1)\n",
    "else:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=scatter_df[\"like_ratio\"], y=scatter_df[\"comment_ratio\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=sizes, color=PASTELS[4], opacity=0.9, line=dict(color=\"#666\", width=0.4)),\n",
    "        text=_wrap(scatter_df[title_col] if title_col in scatter_df.columns else scatter_df[\"videoId\"]),\n",
    "        hovertemplate=\"<b>%{text}</b><br>like/view=%{x:.2%}<br>comment/view=%{y:.3%}<extra></extra>\",\n",
    "        showlegend=False\n",
    "    ), row=4, col=1)\n",
    "    fig.update_xaxes(title_text=\"Like / View\", tickformat=\".0%\", automargin=True, row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Comment / View\", tickformat=\".2%\", automargin=True, row=4, col=1)\n",
    "\n",
    "#  Rows 5 & 6: Tables (give them lots of space, wrap long titles) ----\n",
    "def table_trace(df_slice):\n",
    "    cols = list(df_slice.columns)\n",
    "    # make the title column wider; others compact\n",
    "    if title_col and title_col in cols:\n",
    "        widths = []\n",
    "        for c in cols:\n",
    "            if c == title_col:\n",
    "                widths.append(0.46)\n",
    "            elif c in (\"videoId\", cat_col):\n",
    "                widths.append(0.16)\n",
    "            else:\n",
    "                widths.append(0.09)\n",
    "    else:\n",
    "        widths = [1.0/len(cols)]*len(cols)\n",
    "\n",
    "    return go.Table(\n",
    "        header=dict(values=[f\"<b>{c}</b>\" for c in cols],\n",
    "                    fill_color=\"#f7f7f7\", align=\"left\", font=dict(size=12)),\n",
    "        cells=dict(values=[df_slice[c] for c in cols],\n",
    "                   fill_color=\"white\", align=\"left\", font=dict(size=11), height=26),\n",
    "        columnwidth=widths\n",
    "    )\n",
    "\n",
    "if table_df.empty:\n",
    "    fig.add_trace(go.Table(header=dict(values=[\"(no videos in system for this creator)\"]),\n",
    "                           cells=dict(values=[[\"\"]]), columnwidth=[1.0]),\n",
    "                  row=5, col=1)\n",
    "    fig.add_trace(go.Table(header=dict(values=[\" \"]), cells=dict(values=[[\" \"]])),\n",
    "                  row=6, col=1)\n",
    "else:\n",
    "    half = (len(table_df) + 1)//2\n",
    "    left = table_df.iloc[:half]\n",
    "    right = table_df.iloc[half:]\n",
    "    fig.add_trace(table_trace(left),  row=5, col=1)\n",
    "    fig.add_trace(table_trace(right) if not right.empty\n",
    "                  else go.Table(header=dict(values=[\" \"]), cells=dict(values=[[\" \"]])),\n",
    "                  row=6, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1100, height=2000,        # tall & narrow-ish to reduce crowding\n",
    "    title=dict(text=f\"Creator Insights — ID: {CREATOR_ID}\", x=0.5, xanchor=\"center\"),\n",
    "    margin=dict(l=70, r=70, t=90, b=60),\n",
    "    showlegend=False,\n",
    "    uniformtext_minsize=10, uniformtext_mode=\"hide\",\n",
    ")\n",
    "\n",
    "row_title_y = {\n",
    "    1: 1.975,   # above row 1\n",
    "    2: 1.835,   # above row 2\n",
    "    3: 1.655,   # above row 3\n",
    "    4: 1.460,   # above row 4\n",
    "    5: 1.265,   # above row 5\n",
    "    6: 1.085,   # above row 6\n",
    "}\n",
    "\n",
    "def add_row_title(fig, row, text):\n",
    "    fig.add_annotation(\n",
    "        text=f\"<b>{text}</b>\",\n",
    "        x=0.5, y=row_title_y.get(row, 0.5),\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=False\n",
    "    )\n",
    "\n",
    "add_row_title(fig, 1, \"Creator Overview\")\n",
    "add_row_title(fig, 3, \"Videos by Category\")\n",
    "add_row_title(fig, 4, \"Engagement Map (Like/View vs Comment/View)\")\n",
    "add_row_title(fig, 5, \"Videos (list)\")\n",
    "add_row_title(fig, 6, \"Videos (list continued)\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ID = 85557\n",
    "VIDEOS_CSV = \"videos_scored.csv\"\n",
    "\n",
    "creator_name = \"LiVing Ash\"\n",
    "# in future, replace with a map of the creator ID to the channel name if the name is in the data\n",
    "\n",
    "def _as_num(s, dtype=float):\n",
    "    out = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return out.astype(dtype) if dtype in (int, np.int64) else out\n",
    "\n",
    "def iso8601_to_seconds(s: str):\n",
    "    if not isinstance(s, str) or not s:\n",
    "        return None\n",
    "    m = re.fullmatch(\n",
    "        r\"P(?:(?P<days>\\d+)D)?(?:T(?:(?P<hours>\\d+)H)?(?:(?P<minutes>\\d+)M)?(?:(?P<seconds>\\d+)S)?)?\",\n",
    "        s.strip()\n",
    "    )\n",
    "    if not m:\n",
    "        m = re.fullmatch(r\"PT(?:(?P<hours>\\d+)H)?(?:(?P<minutes>\\d+)M)?(?:(?P<seconds>\\d+)S)?\", s.strip())\n",
    "        if not m:\n",
    "            return None\n",
    "    d = {k: (int(v) if v else 0) for k, v in m.groupdict().items()}\n",
    "    total = d.get(\"days\", 0)*86400 + d.get(\"hours\", 0)*3600 + d.get(\"minutes\", 0)*60 + d.get(\"seconds\", 0)\n",
    "    return float(total)\n",
    "\n",
    "def human_duration(seconds: float | None) > str:\n",
    "    if seconds is None or np.isnan(seconds):\n",
    "        return \"unknown\"\n",
    "    seconds = int(round(seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    parts = []\n",
    "    if h > 0:\n",
    "        parts.append(f\"{h}H\")\n",
    "    if m > 0 or h > 0: \n",
    "        parts.append(f\"{m}M\")\n",
    "    parts.append(f\"{s}S\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def short_or_long(seconds: float | None) > str:\n",
    "    if seconds is None or np.isnan(seconds):\n",
    "        return \"unknown\"\n",
    "    return \"shortform\" if seconds <= 180 else \"longform\"\n",
    "\n",
    "def pick_value(*vals, fallback=\"unknown\"):\n",
    "    for v in vals:\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v\n",
    "    return fallback\n",
    "\n",
    "def pct_or_blank(x, places=2):\n",
    "    try:\n",
    "        return f\"{float(x):.{places}%}\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def int_or_zero(x):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def video_insights(video_id: str, videos_csv: str = VIDEOS_CSV, creator_name_map=None):\n",
    "    creator_name_map = creator_name_map or {}\n",
    "\n",
    "    df = pd.read_csv(videos_csv, dtype=str, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    if \"videoId\" not in df.columns:\n",
    "        for alt in [\"video_id\", \"id\", \"VideoID\"]:\n",
    "            if alt in df.columns:\n",
    "                df.rename(columns={alt: \"videoId\"}, inplace=True)\n",
    "                break\n",
    "\n",
    "    subset = df.loc[df[\"videoId\"].astype(str).str.strip() == str(video_id).strip()]\n",
    "    if subset.empty:\n",
    "        print(f\"No row found for videoId={video_id!r} in {videos_csv}.\")\n",
    "        return\n",
    "    if len(subset) > 1:\n",
    "        # if duplicates, take the first but show how many\n",
    "        print(f\"Warning: found {len(subset)} rows for videoId={video_id}; showing the first.\\n\")\n",
    "    row = subset.iloc[0]\n",
    "\n",
    "    # Basic details\n",
    "    channel_id = row.get(\"channelId\", \"\")\n",
    "\n",
    "    category = pick_value(row.get(\"defining_category\", \"\"), row.get(\"category\", \"\"))\n",
    "    title    = row.get(\"title\", row.get(\"video_title\", \"\"))\n",
    "    tags     = row.get(\"tags\", \"\")\n",
    "    duration_iso = row.get(\"contentDuration\", row.get(\"duration\", \"\"))\n",
    "    seconds = iso8601_to_seconds(duration_iso) if duration_iso else None\n",
    "    duration_h = human_duration(seconds)\n",
    "    vid_class = short_or_long(seconds)\n",
    "\n",
    "    default_lang = pick_value(row.get(\"defaultLanguage\", \"\"), fallback=\"unknown\")\n",
    "    default_audio = pick_value(row.get(\"defaultAudioLanguage\", \"\"), fallback=\"unknown\")\n",
    "\n",
    "    views   = int_or_zero(row.get(\"viewCount\", \"0\"))\n",
    "    likes   = int_or_zero(row.get(\"likeCount\", \"0\"))\n",
    "    commentCount_meta = int_or_zero(row.get(\"commentCount\", \"0\"))\n",
    "    total_comment_count = int_or_zero(row.get(\"total_comment_count\", row.get(\"normal_comment_count\", \"0\")))\n",
    "\n",
    "    like_ratio = pd.to_numeric(row.get(\"like_ratio\", \"nan\"), errors=\"coerce\")\n",
    "    if pd.isna(like_ratio):\n",
    "        like_ratio = (likes / views) if views > 0 else np.nan\n",
    "\n",
    "    comment_ratio = pd.to_numeric(row.get(\"comment_ratio\", \"nan\"), errors=\"coerce\")\n",
    "    if pd.isna(comment_ratio):\n",
    "        comments_used = max(commentCount_meta, total_comment_count)\n",
    "        comment_ratio = (comments_used / views) if views > 0 else np.nan\n",
    "\n",
    "    video_score = pd.to_numeric(row.get(\"video_score\", \"nan\"), errors=\"coerce\")\n",
    "    spam_ratio  = pd.to_numeric(row.get(\"spam_ratio\", \"nan\"), errors=\"coerce\")\n",
    "    cmt_mean    = pd.to_numeric(row.get(\"comment_score_mean\", \"nan\"), errors=\"coerce\")\n",
    "    cmt_min     = pd.to_numeric(row.get(\"comment_score_min\", \"nan\"), errors=\"coerce\")\n",
    "    cmt_max     = pd.to_numeric(row.get(\"comment_score_max\", \"nan\"), errors=\"coerce\")\n",
    "\n",
    "    print(\"\\nVideo details:\")\n",
    "    print(f\"  videoId:               {video_id}\")\n",
    "    print(f\"  channelId:             {channel_id}\")\n",
    "    print(f\"  Creator name:          {creator_name}\")\n",
    "    print(f\"  category:              {category}\")\n",
    "    print(f\"  title:                 {title}\")\n",
    "    print(f\"  tags:                  {tags}\")\n",
    "    print(f\"  duration:              {duration_h}\")\n",
    "    print(f\"  video classification:  {vid_class}\")\n",
    "    print(f\"  defaultLanguage:       {default_lang}\")\n",
    "    print(f\"  defaultAudioLanguage:  {default_audio}\")\n",
    "\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"  viewCount:             {views:,}\")\n",
    "    print(f\"  likeCount:             {likes:,}\")\n",
    "    print(f\"  commentCount:          {commentCount_meta:,}\")\n",
    "\n",
    "    print(\"\\nEngagement:\")\n",
    "    print(f\"  Video score:              {'' if pd.isna(video_score) else f'{video_score:.3f}'}\")\n",
    "    print(f\"  Comments in the system:   {total_comment_count:,}\")\n",
    "    print(f\"  Spam ratio:               {pct_or_blank(spam_ratio, 2)}\")\n",
    "    print(f\"  Comment score mean:       {'' if pd.isna(cmt_mean) else f'{cmt_mean:.3f}'}\")\n",
    "    print(f\"  Comment score range:   \"\n",
    "          f\"{'' if pd.isna(cmt_min) else f'{cmt_min:.3f}'} to {'' if pd.isna(cmt_max) else f'{cmt_max:.3f}'}\")\n",
    "    print(f\"  Like-view ratio:            {pct_or_blank(like_ratio, 2)}\")\n",
    "    print(f\"  Comment-view ratio:         {pct_or_blank(comment_ratio, 3)}\")\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_insights(VIDEO_ID, VIDEOS_CSV, creator_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LorealDatathon (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
